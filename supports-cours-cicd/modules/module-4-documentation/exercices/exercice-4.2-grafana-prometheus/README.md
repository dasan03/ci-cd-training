# Exercice 4.2 - Configuration de dashboards avec Grafana et Prometheus

## Objectifs

Ã€ l'issue de cet exercice, vous serez capable de :
- Configurer une stack Prometheus/Grafana avec Docker Compose
- Exposer des mÃ©triques de tests personnalisÃ©es avec Prometheus
- CrÃ©er des dashboards interactifs dans Grafana
- Configurer des alertes sur les mÃ©triques de tests
- Analyser les performances et la qualitÃ© des tests en temps rÃ©el

## DurÃ©e estimÃ©e
60 minutes

## PrÃ©requis

- Docker et Docker Compose installÃ©s
- Node.js (version 16+) ou Python (version 3.8+)
- Navigateur web moderne
- Connaissances de base en mÃ©triques et monitoring

## Contexte

Votre Ã©quipe souhaite mettre en place un monitoring en temps rÃ©el des tests automatisÃ©s. L'objectif est de crÃ©er des dashboards qui permettent de suivre les performances, la stabilitÃ© et la qualitÃ© des tests, avec des alertes automatiques en cas de dÃ©gradation.

## Ã‰tape 1 : Configuration de la stack Prometheus/Grafana

### 1.1 Structure du projet

CrÃ©ez la structure suivante :

```
grafana-prometheus-demo/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ prometheus/
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â””â”€â”€ alert-rules.yml
â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â”‚   â””â”€â”€ prometheus.yml
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â”œâ”€â”€ dashboard.yml
â”‚   â”‚       â””â”€â”€ test-dashboard.json
â”œâ”€â”€ test-app/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ metrics-server.js
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ load-test.js
â””â”€â”€ alertmanager/
    â””â”€â”€ alertmanager.yml
```

### 1.2 Configuration Docker Compose

CrÃ©ez le fichier `docker-compose.yml` :

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/alert-rules.yml:/etc/prometheus/alert-rules.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--web.enable-remote-write-receiver'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    networks:
      - monitoring

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - monitoring

  test-metrics:
    build: ./test-app
    container_name: test-metrics
    ports:
      - "8000:8000"
    environment:
      - NODE_ENV=production
    networks:
      - monitoring

volumes:
  prometheus-data:
  grafana-data:

networks:
  monitoring:
    driver: bridge
```

### 1.3 Configuration Prometheus

CrÃ©ez le fichier `prometheus/prometheus.yml` :

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert-rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'test-metrics'
    static_configs:
      - targets: ['test-metrics:8000']
    scrape_interval: 5s
    metrics_path: '/metrics'
    
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 15s

  - job_name: 'grafana'
    static_configs:
      - targets: ['grafana:3000']
    scrape_interval: 30s
```

## Ã‰tape 2 : Application de test avec mÃ©triques

### 2.1 Configuration Node.js

CrÃ©ez le fichier `test-app/package.json` :

```json
{
  "name": "test-metrics-app",
  "version": "1.0.0",
  "description": "Application de test avec mÃ©triques Prometheus",
  "main": "src/metrics-server.js",
  "scripts": {
    "start": "node src/metrics-server.js",
    "test": "node tests/load-test.js",
    "dev": "nodemon src/metrics-server.js"
  },
  "dependencies": {
    "express": "^4.18.0",
    "prom-client": "^14.2.0",
    "axios": "^1.4.0"
  },
  "devDependencies": {
    "nodemon": "^2.0.22"
  }
}
```

### 2.2 Serveur de mÃ©triques

CrÃ©ez le fichier `test-app/src/metrics-server.js` :

```javascript
const express = require('express');
const client = require('prom-client');

const app = express();
const port = process.env.PORT || 8000;

// CrÃ©ation du registre Prometheus
const register = new client.Registry();

// MÃ©triques par dÃ©faut (CPU, mÃ©moire, etc.)
client.collectDefaultMetrics({ register });

// MÃ©triques personnalisÃ©es pour les tests
const testCounter = new client.Counter({
  name: 'tests_total',
  help: 'Total number of tests executed',
  labelNames: ['status', 'suite', 'environment', 'test_type'],
  registers: [register]
});

const testDuration = new client.Histogram({
  name: 'test_duration_seconds',
  help: 'Time spent executing tests',
  labelNames: ['test_name', 'suite', 'test_type'],
  buckets: [0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, 120.0],
  registers: [register]
});

const activeTests = new client.Gauge({
  name: 'active_tests_count',
  help: 'Number of currently running tests',
  registers: [register]
});

const testQueueSize = new client.Gauge({
  name: 'test_queue_size',
  help: 'Number of tests waiting to be executed',
  registers: [register]
});

const testCoverage = new client.Gauge({
  name: 'test_coverage_percent',
  help: 'Code coverage percentage',
  labelNames: ['type'],
  registers: [register]
});

const testFlakiness = new client.Gauge({
  name: 'test_flakiness_rate',
  help: 'Test flakiness rate percentage',
  labelNames: ['suite'],
  registers: [register]
});

const buildInfo = new client.Gauge({
  name: 'build_info',
  help: 'Build information',
  labelNames: ['version', 'commit', 'branch'],
  registers: [register]
});

// Simulation de donnÃ©es de build
buildInfo.set({
  version: process.env.BUILD_VERSION || '1.0.0',
  commit: process.env.GIT_COMMIT || 'abc123',
  branch: process.env.GIT_BRANCH || 'main'
}, 1);

// Classe pour simuler l'exÃ©cution de tests
class TestSimulator {
  constructor() {
    this.testSuites = ['unit', 'integration', 'e2e', 'performance'];
    this.testTypes = ['smoke', 'regression', 'api', 'ui'];
    this.environments = ['dev', 'staging', 'prod'];
    this.isRunning = false;
  }

  startSimulation() {
    if (this.isRunning) return;
    this.isRunning = true;

    // Simulation continue de tests
    setInterval(() => {
      this.simulateTestExecution();
    }, 2000);

    // Mise Ã  jour des mÃ©triques de couverture
    setInterval(() => {
      this.updateCoverageMetrics();
    }, 30000);

    // Mise Ã  jour de la flakiness
    setInterval(() => {
      this.updateFlakinessMetrics();
    }, 60000);

    console.log('ğŸ§ª Simulation de tests dÃ©marrÃ©e');
  }

  simulateTestExecution() {
    const suite = this.getRandomElement(this.testSuites);
    const testType = this.getRandomElement(this.testTypes);
    const environment = this.getRandomElement(this.environments);
    const testName = `test_${Math.floor(Math.random() * 100)}`;

    // Simulation du temps d'exÃ©cution
    const executionTime = this.generateExecutionTime(suite);
    
    // Simulation du rÃ©sultat
    const status = this.generateTestResult(suite);

    // Mise Ã  jour des mÃ©triques
    activeTests.inc();
    testQueueSize.set(Math.floor(Math.random() * 20));

    setTimeout(() => {
      testCounter.labels(status, suite, environment, testType).inc();
      testDuration.labels(testName, suite, testType).observe(executionTime);
      activeTests.dec();
    }, executionTime * 1000);
  }

  generateExecutionTime(suite) {
    const baseTimes = {
      'unit': 0.5,
      'integration': 2.0,
      'e2e': 8.0,
      'performance': 30.0
    };
    
    const baseTime = baseTimes[suite] || 1.0;
    return baseTime + (Math.random() * baseTime * 0.5);
  }

  generateTestResult(suite) {
    const failureRates = {
      'unit': 0.05,
      'integration': 0.10,
      'e2e': 0.15,
      'performance': 0.20
    };

    const failureRate = failureRates[suite] || 0.10;
    const random = Math.random();

    if (random < failureRate * 0.3) return 'skipped';
    if (random < failureRate) return 'failed';
    return 'passed';
  }

  updateCoverageMetrics() {
    testCoverage.set({ type: 'line' }, 85 + Math.random() * 10);
    testCoverage.set({ type: 'branch' }, 80 + Math.random() * 15);
    testCoverage.set({ type: 'function' }, 90 + Math.random() * 8);
  }

  updateFlakinessMetrics() {
    this.testSuites.forEach(suite => {
      const flakiness = Math.random() * 5; // 0-5% flakiness
      testFlakiness.set({ suite }, flakiness);
    });
  }

  getRandomElement(array) {
    return array[Math.floor(Math.random() * array.length)];
  }
}

// Routes API
app.get('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType);
  res.end(await register.metrics());
});

app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    uptime: process.uptime()
  });
});

app.get('/api/test-stats', (req, res) => {
  res.json({
    activeTests: activeTests.get(),
    queueSize: testQueueSize.get(),
    timestamp: new Date().toISOString()
  });
});

// Endpoint pour dÃ©clencher des tests manuellement
app.post('/api/trigger-test', (req, res) => {
  const { suite = 'unit', count = 1 } = req.body;
  
  for (let i = 0; i < count; i++) {
    setTimeout(() => {
      simulator.simulateTestExecution();
    }, i * 100);
  }
  
  res.json({
    message: `Triggered ${count} tests for suite ${suite}`,
    timestamp: new Date().toISOString()
  });
});

// DÃ©marrage du serveur
const simulator = new TestSimulator();

app.listen(port, () => {
  console.log(`ğŸš€ Serveur de mÃ©triques dÃ©marrÃ© sur le port ${port}`);
  console.log(`ğŸ“Š MÃ©triques disponibles sur http://localhost:${port}/metrics`);
  console.log(`ğŸ¥ Health check sur http://localhost:${port}/health`);
  
  simulator.startSimulation();
});

// Gestion propre de l'arrÃªt
process.on('SIGTERM', () => {
  console.log('ğŸ›‘ ArrÃªt du serveur...');
  process.exit(0);
});
```

### 2.3 Dockerfile pour l'application

CrÃ©ez le fichier `test-app/Dockerfile` :

```dockerfile
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production

COPY src/ ./src/

EXPOSE 8000

USER node

CMD ["npm", "start"]
```

## Ã‰tape 3 : Configuration Grafana

### 3.1 Source de donnÃ©es Prometheus

CrÃ©ez le fichier `grafana/provisioning/datasources/prometheus.yml` :

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      timeInterval: "5s"
      queryTimeout: "60s"
      httpMethod: "POST"
```

### 3.2 Configuration des dashboards

CrÃ©ez le fichier `grafana/provisioning/dashboards/dashboard.yml` :

```yaml
apiVersion: 1

providers:
  - name: 'Test Dashboards'
    orgId: 1
    folder: 'Tests'
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

### 3.3 Dashboard principal

CrÃ©ez le fichier `grafana/provisioning/dashboards/test-dashboard.json` :

```json
{
  "dashboard": {
    "id": null,
    "title": "Test Execution Dashboard",
    "tags": ["testing", "ci-cd", "quality"],
    "timezone": "browser",
    "refresh": "5s",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "id": 1,
        "title": "Test Success Rate",
        "type": "stat",
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "rate(tests_total{status=\"passed\"}[5m]) / rate(tests_total[5m]) * 100",
            "legendFormat": "Success Rate",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100,
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 80},
                {"color": "green", "value": 95}
              ]
            },
            "mappings": [],
            "custom": {
              "displayMode": "basic"
            }
          }
        },
        "options": {
          "reduceOptions": {
            "values": false,
            "calcs": ["lastNotNull"],
            "fields": ""
          },
          "orientation": "auto",
          "textMode": "auto",
          "colorMode": "background",
          "graphMode": "area",
          "justifyMode": "auto"
        }
      },
      {
        "id": 2,
        "title": "Active Tests",
        "type": "stat",
        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0},
        "targets": [
          {
            "expr": "active_tests_count",
            "legendFormat": "Active Tests",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "thresholds": {
              "steps": [
                {"color": "green", "value": 0},
                {"color": "yellow", "value": 10},
                {"color": "red", "value": 20}
              ]
            }
          }
        }
      },
      {
        "id": 3,
        "title": "Test Queue Size",
        "type": "stat",
        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "test_queue_size",
            "legendFormat": "Queue Size",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "thresholds": {
              "steps": [
                {"color": "green", "value": 0},
                {"color": "yellow", "value": 5},
                {"color": "red", "value": 15}
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "title": "Test Coverage",
        "type": "stat",
        "gridPos": {"h": 8, "w": 6, "x": 18, "y": 0},
        "targets": [
          {
            "expr": "test_coverage_percent{type=\"line\"}",
            "legendFormat": "Line Coverage",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100,
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 70},
                {"color": "green", "value": 85}
              ]
            }
          }
        }
      },
      {
        "id": 5,
        "title": "Test Execution Rate",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [
          {
            "expr": "rate(tests_total[1m])",
            "legendFormat": "Tests/sec ({{status}})",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "reqps",
            "custom": {
              "drawStyle": "line",
              "lineInterpolation": "linear",
              "fillOpacity": 10,
              "gradientMode": "none",
              "showPoints": "never",
              "pointSize": 5,
              "stacking": {"mode": "none", "group": "A"},
              "axisPlacement": "auto",
              "axisLabel": "",
              "scaleDistribution": {"type": "linear"}
            }
          }
        }
      },
      {
        "id": 6,
        "title": "Test Duration Percentiles",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(test_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.50, rate(test_duration_seconds_bucket[5m]))",
            "legendFormat": "Median",
            "refId": "B"
          },
          {
            "expr": "histogram_quantile(0.99, rate(test_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile",
            "refId": "C"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "custom": {
              "drawStyle": "line",
              "lineInterpolation": "linear",
              "fillOpacity": 0,
              "gradientMode": "none"
            }
          }
        }
      },
      {
        "id": 7,
        "title": "Tests by Suite",
        "type": "piechart",
        "gridPos": {"h": 8, "w": 8, "x": 0, "y": 16},
        "targets": [
          {
            "expr": "sum by (suite) (rate(tests_total[5m]))",
            "legendFormat": "{{suite}}",
            "refId": "A"
          }
        ],
        "options": {
          "reduceOptions": {
            "values": false,
            "calcs": ["lastNotNull"],
            "fields": ""
          },
          "pieType": "pie",
          "tooltip": {"mode": "single"},
          "legend": {
            "displayMode": "visible",
            "placement": "right"
          }
        }
      },
      {
        "id": 8,
        "title": "Test Results Distribution",
        "type": "piechart",
        "gridPos": {"h": 8, "w": 8, "x": 8, "y": 16},
        "targets": [
          {
            "expr": "sum by (status) (rate(tests_total[5m]))",
            "legendFormat": "{{status}}",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "custom": {
              "hideFrom": {
                "legend": false,
                "tooltip": false,
                "vis": false
              }
            },
            "mappings": [],
            "color": {
              "mode": "palette-classic"
            }
          },
          "overrides": [
            {
              "matcher": {"id": "byName", "options": "passed"},
              "properties": [{"id": "color", "value": {"mode": "fixed", "fixedColor": "green"}}]
            },
            {
              "matcher": {"id": "byName", "options": "failed"},
              "properties": [{"id": "color", "value": {"mode": "fixed", "fixedColor": "red"}}]
            },
            {
              "matcher": {"id": "byName", "options": "skipped"},
              "properties": [{"id": "color", "value": {"mode": "fixed", "fixedColor": "yellow"}}]
            }
          ]
        }
      },
      {
        "id": 9,
        "title": "Test Flakiness by Suite",
        "type": "bargauge",
        "gridPos": {"h": 8, "w": 8, "x": 16, "y": 16},
        "targets": [
          {
            "expr": "test_flakiness_rate",
            "legendFormat": "{{suite}}",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 10,
            "thresholds": {
              "steps": [
                {"color": "green", "value": 0},
                {"color": "yellow", "value": 2},
                {"color": "red", "value": 5}
              ]
            }
          }
        },
        "options": {
          "orientation": "horizontal",
          "displayMode": "gradient"
        }
      }
    ]
  }
}
```

## Ã‰tape 4 : Configuration des alertes

### 4.1 RÃ¨gles d'alerte Prometheus

CrÃ©ez le fichier `prometheus/alert-rules.yml` :

```yaml
groups:
- name: test-alerts
  rules:
  - alert: HighTestFailureRate
    expr: rate(tests_total{status="failed"}[5m]) / rate(tests_total[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
      team: qa
    annotations:
      summary: "High test failure rate detected"
      description: "Test failure rate is {{ $value | humanizePercentage }} over the last 5 minutes"
      runbook_url: "https://wiki.company.com/runbooks/high-test-failure-rate"

  - alert: SlowTestExecution
    expr: histogram_quantile(0.95, rate(test_duration_seconds_bucket[5m])) > 60
    for: 5m
    labels:
      severity: warning
      team: qa
    annotations:
      summary: "Tests are running slowly"
      description: "95th percentile test execution time is {{ $value }}s"

  - alert: TestSuiteDown
    expr: up{job="test-metrics"} == 0
    for: 1m
    labels:
      severity: critical
      team: devops
    annotations:
      summary: "Test suite monitoring is down"
      description: "The test metrics service is not responding"

  - alert: HighTestFlakiness
    expr: test_flakiness_rate > 5
    for: 10m
    labels:
      severity: warning
      team: qa
    annotations:
      summary: "High test flakiness detected"
      description: "Test suite {{ $labels.suite }} has {{ $value }}% flakiness rate"

  - alert: LowTestCoverage
    expr: test_coverage_percent{type="line"} < 70
    for: 5m
    labels:
      severity: warning
      team: dev
    annotations:
      summary: "Low test coverage detected"
      description: "Line coverage is {{ $value }}%, below the 70% threshold"

  - alert: TestQueueBacklog
    expr: test_queue_size > 20
    for: 3m
    labels:
      severity: warning
      team: devops
    annotations:
      summary: "Test queue backlog detected"
      description: "{{ $value }} tests are waiting in the queue"
```

### 4.2 Configuration AlertManager

CrÃ©ez le fichier `alertmanager/alertmanager.yml` :

```yaml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@company.com'
  smtp_auth_username: 'alerts@company.com'
  smtp_auth_password: 'password'

route:
  group_by: ['alertname', 'team']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      team: qa
    receiver: 'qa-team'
  - match:
      team: devops
    receiver: 'devops-team'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://localhost:5001/webhook'
    send_resolved: true

- name: 'critical-alerts'
  email_configs:
  - to: 'oncall@company.com'
    subject: 'CRITICAL: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      {{ end }}
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts-critical'
    title: 'Critical Test Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

- name: 'qa-team'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#qa-alerts'
    title: 'QA Test Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

- name: 'devops-team'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#devops-alerts'
    title: 'DevOps Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'team']
```

## Ã‰tape 5 : DÃ©marrage et test de la stack

### 5.1 DÃ©marrage des services

```bash
# DÃ©marrage de tous les services
docker-compose up -d

# VÃ©rification du statut
docker-compose ps

# Consultation des logs
docker-compose logs -f test-metrics
```

### 5.2 VÃ©rification des services

1. **Prometheus** : http://localhost:9090
   - VÃ©rifiez les targets dans Status > Targets
   - Testez quelques requÃªtes PromQL

2. **Grafana** : http://localhost:3000 (admin/admin123)
   - VÃ©rifiez la source de donnÃ©es Prometheus
   - Explorez le dashboard de tests

3. **AlertManager** : http://localhost:9093
   - VÃ©rifiez la configuration
   - Consultez les alertes actives

4. **MÃ©triques de test** : http://localhost:8000/metrics
   - VÃ©rifiez que les mÃ©triques sont exposÃ©es

## Ã‰tape 6 : Personnalisation et analyse

### 6.1 CrÃ©ation d'un dashboard personnalisÃ©

Dans Grafana, crÃ©ez un nouveau dashboard avec :

1. **Panel de mÃ©triques systÃ¨me** :
   - CPU et mÃ©moire du serveur de tests
   - Utilisation disque

2. **Panel de performance** :
   - Temps de rÃ©ponse par endpoint
   - Throughput des tests

3. **Panel de qualitÃ©** :
   - Ã‰volution de la couverture de code
   - Tendance des Ã©checs par jour

### 6.2 Configuration d'alertes personnalisÃ©es

Ajoutez des alertes pour :
- DÃ©gradation de performance (> 20% d'augmentation du temps d'exÃ©cution)
- Baisse de couverture de code (< seuil dÃ©fini)
- Augmentation du taux d'Ã©chec sur une pÃ©riode

### 6.3 RequÃªtes PromQL utiles

```promql
# Taux de rÃ©ussite par environnement
sum(rate(tests_total{status="passed"}[5m])) by (environment) / 
sum(rate(tests_total[5m])) by (environment) * 100

# Tests les plus lents
topk(10, histogram_quantile(0.95, 
  sum(rate(test_duration_seconds_bucket[5m])) by (test_name, le)
))

# Ã‰volution du nombre de tests par jour
increase(tests_total[1d])

# DÃ©tection d'anomalies dans les temps d'exÃ©cution
test_duration_seconds > on() group_left() (
  avg_over_time(test_duration_seconds[1h]) + 
  2 * stddev_over_time(test_duration_seconds[1h])
)
```

## Ã‰tape 7 : IntÃ©gration avec CI/CD

### 7.1 Script de dÃ©ploiement

CrÃ©ez le fichier `deploy-monitoring.sh` :

```bash
#!/bin/bash

echo "ğŸš€ DÃ©ploiement de la stack de monitoring..."

# VÃ©rification des prÃ©requis
if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose n'est pas installÃ©"
    exit 1
fi

# ArrÃªt des services existants
echo "ğŸ›‘ ArrÃªt des services existants..."
docker-compose down

# Nettoyage des volumes (optionnel)
read -p "Voulez-vous nettoyer les donnÃ©es existantes ? (y/N): " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    docker-compose down -v
    echo "ğŸ§¹ DonnÃ©es nettoyÃ©es"
fi

# Construction et dÃ©marrage
echo "ğŸ—ï¸ Construction et dÃ©marrage des services..."
docker-compose up -d --build

# Attente que les services soient prÃªts
echo "â³ Attente des services..."
sleep 30

# VÃ©rification de la santÃ© des services
echo "ğŸ¥ VÃ©rification de la santÃ© des services..."

services=("prometheus:9090" "grafana:3000" "test-metrics:8000")
for service in "${services[@]}"; do
    IFS=':' read -r name port <<< "$service"
    if curl -f "http://localhost:$port/health" &>/dev/null || 
       curl -f "http://localhost:$port" &>/dev/null; then
        echo "âœ… $name est opÃ©rationnel"
    else
        echo "âŒ $name n'est pas accessible"
    fi
done

echo "ğŸŒ Services disponibles :"
echo "ğŸ“Š Grafana: http://localhost:3000 (admin/admin123)"
echo "ğŸ” Prometheus: http://localhost:9090"
echo "ğŸš¨ AlertManager: http://localhost:9093"
echo "ğŸ“ˆ MÃ©triques: http://localhost:8000/metrics"

echo "âœ¨ DÃ©ploiement terminÃ© !"
```

## Questions d'analyse

1. **MÃ©triques de performance** :
   - Quels sont les tests les plus lents ?
   - Y a-t-il des patterns dans les temps d'exÃ©cution ?
   - Comment Ã©volue la performance dans le temps ?

2. **QualitÃ© des tests** :
   - Quel est le taux de rÃ©ussite par suite de tests ?
   - Quels sont les tests les plus instables ?
   - Comment Ã©volue la couverture de code ?

3. **Alerting** :
   - Quelles alertes sont les plus frÃ©quentes ?
   - Y a-t-il des faux positifs Ã  Ã©liminer ?
   - Les seuils d'alerte sont-ils appropriÃ©s ?

## Livrables attendus

1. **Stack complÃ¨te** Prometheus/Grafana fonctionnelle
2. **Dashboards personnalisÃ©s** avec mÃ©triques pertinentes
3. **Alertes configurÃ©es** avec notifications
4. **Documentation** des mÃ©triques et seuils
5. **Analyse** des rÃ©sultats et recommandations

## Ressources complÃ©mentaires

- [Documentation Prometheus](https://prometheus.io/docs/)
- [Documentation Grafana](https://grafana.com/docs/)
- [PromQL Guide](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [Grafana Dashboard Examples](https://grafana.com/grafana/dashboards/)

## DÃ©pannage

### ProblÃ¨mes courants

1. **Services non accessibles** :
   ```bash
   docker-compose logs [service-name]
   docker-compose ps
   ```

2. **MÃ©triques non collectÃ©es** :
   - VÃ©rifiez la configuration Prometheus
   - ContrÃ´lez les targets dans l'interface Prometheus

3. **Dashboards vides** :
   - VÃ©rifiez la source de donnÃ©es dans Grafana
   - Testez les requÃªtes PromQL manuellement

4. **Alertes non dÃ©clenchÃ©es** :
   - VÃ©rifiez les rÃ¨gles d'alerte
   - ContrÃ´lez la configuration AlertManager