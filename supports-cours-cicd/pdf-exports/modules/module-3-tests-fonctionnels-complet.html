<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Module 3 - Tests Fonctionnels et Non-Fonctionnels</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 2cm; line-height: 1.6; }
        h1 { color: #2c3e50; border-bottom: 2px solid #3498db; }
        h2 { color: #34495e; margin-top: 2em; }
        h3 { color: #7f8c8d; }
        code { background: #f8f9fa; padding: 2px 4px; border-radius: 3px; }
        pre { background: #f8f9fa; padding: 1em; border-radius: 5px; overflow-x: auto; }
        li { margin: 0.5em 0; }
        @media print {
            body { margin: 1cm; }
            h1 { page-break-before: always; }
        }
    </style>
</head>
<body>
    <h1>Module 3 - Tests Fonctionnels et Non-Fonctionnels</h1>
    <h1>Module 3 - Tests Fonctionnels et Non-Fonctionnels</h1><br><br><h1>Module 3 : Tests fonctionnels et non fonctionnels dans un pipeline CI/CD</h1><br><br><h2>Objectifs du module</h2><br><li>Exécuter des tests fonctionnels et non fonctionnels dans un pipeline automatisé</li><br><li>Assurer la qualité logicielle en intégrant des tests de sécurité et de performance</li><br><br><h2>Durée</h2><br>6 heures (1,5 jour)<br><br><h2>Prérequis</h2><br><li>Environnements de test cloud (SauceLabs, BrowserStack)</li><br><li>Frameworks de test de charge (JMeter, Gatling)</li><br><li>Outils de scan de sécurité (OWASP ZAP, Burp Suite)</li><br><br><h2>Structure du module</h2><br><li><code>support-theorique/</code> - Contenu des cours et présentations</li><br><li><code>exercices/</code> - Exercices pratiques avec solutions</li><br><li><code>qcm/</code> - Questions d'évaluation intermédiaire</li><br><li><code>ressources/</code> - Fichiers de support et templates</li><br><br>\newpage<br><br><h1>Support Théorique</h1><br><br><h1>1. Tests Fonctionnels Automatisés</h1><br><br><h2>1.1 Introduction aux Tests Fonctionnels</h2><br><br><h3>Définition et Objectifs</h3><br><br>Les tests fonctionnels vérifient que l'application fonctionne conformément aux spécifications métier. Ils valident :<br><li>Les fonctionnalités utilisateur</li><br><li>Les flux de navigation</li><br><li>L'intégration entre composants</li><br><li>La conformité aux exigences</li><br><br><h3>Types de Tests Fonctionnels</h3><br><br><strong>Tests d'Interface Utilisateur (UI)</strong><br><li>Validation des éléments visuels</li><br><li>Vérification des interactions utilisateur</li><br><li>Tests de navigation et de workflow</li><br><br><strong>Tests d'API</strong><br><li>Validation des endpoints REST/GraphQL</li><br><li>Vérification des contrats d'interface</li><br><li>Tests d'intégration entre services</li><br><br><strong>Tests End-to-End (E2E)</strong><br><li>Simulation de parcours utilisateur complets</li><br><li>Validation des flux métier critiques</li><br><li>Tests cross-browser et cross-platform</li><br><br><h2>1.2 Tests UI avec Selenium</h2><br><br><h3>Présentation de Selenium</h3><br><br>Selenium est une suite d'outils pour l'automatisation des navigateurs web :<br><li><strong>Selenium WebDriver</strong> : API pour contrôler les navigateurs</li><br><li><strong>Selenium Grid</strong> : Exécution distribuée des tests</li><br><li><strong>Selenium IDE</strong> : Enregistrement et lecture de tests</li><br><br><h3>Architecture Selenium WebDriver</h3><br><br><pre><code><br>Test Script → WebDriver API → Browser Driver → Browser<br></code></pre><br><br><h3>Avantages de Selenium</h3><br><li>Support multi-navigateurs (Chrome, Firefox, Safari, Edge)</li><br><li>Langages multiples (Java, Python, C#, JavaScript)</li><br><li>Intégration CI/CD native</li><br><li>Communauté active et écosystème riche</li><br><br><h3>Exemple de Test Selenium (JavaScript)</h3><br><br><pre><code>javascript<br>const { Builder, By, until } = require('selenium-webdriver');<br><br>describe('Login Test', () => {<br>  let driver;<br><br>  beforeEach(async () => {<br>    driver = await new Builder().forBrowser('chrome').build();<br>  });<br><br>  afterEach(async () => {<br>    await driver.quit();<br>  });<br><br>  it('should login successfully', async () => {<br>    await driver.get('http://localhost:3000/login');<br>    <br>    await driver.findElement(By.id('username')).sendKeys('testuser');<br>    await driver.findElement(By.id('password')).sendKeys('password123');<br>    await driver.findElement(By.css('button[type="submit"]')).click();<br>    <br>    await driver.wait(until.urlContains('/dashboard'), 5000);<br>    <br>    const title = await driver.getTitle();<br>    expect(title).toContain('Dashboard');<br>  });<br>});<br></code></pre><br><br><h2>1.3 Tests UI avec Cypress</h2><br><br><h3>Présentation de Cypress</h3><br><br>Cypress est un framework de test moderne conçu pour les applications web :<br><li>Exécution dans le navigateur</li><br><li>Debugging en temps réel</li><br><li>Captures d'écran et vidéos automatiques</li><br><li>API intuitive et moderne</li><br><br><h3>Architecture Cypress</h3><br><br><pre><code><br>Test Runner → Cypress App → Browser (même origine)<br></code></pre><br><br><h3>Avantages de Cypress</h3><br><li>Configuration minimale</li><br><li>Debugging interactif</li><br><li>Tests rapides et fiables</li><br><li>Mocking et stubbing intégrés</li><br><li>Time-travel debugging</li><br><br><h3>Exemple de Test Cypress</h3><br><br><pre><code>javascript<br>describe('E-commerce Checkout', () => {<br>  beforeEach(() => {<br>    cy.visit('/products');<br>  });<br><br>  it('should complete purchase flow', () => {<br>    // Ajouter un produit au panier<br>    cy.get('[data-testid="product-1"]').click();<br>    cy.get('[data-testid="add-to-cart"]').click();<br>    <br>    // Aller au panier<br>    cy.get('[data-testid="cart-icon"]').click();<br>    cy.url().should('include', '/cart');<br>    <br>    // Procéder au checkout<br>    cy.get('[data-testid="checkout-btn"]').click();<br>    <br>    // Remplir les informations<br>    cy.get('#email').type('user@example.com');<br>    cy.get('#address').type('123 Test Street');<br>    cy.get('#payment-method').select('credit-card');<br>    <br>    // Confirmer la commande<br>    cy.get('[data-testid="confirm-order"]').click();<br>    <br>    // Vérifier la confirmation<br>    cy.contains('Order confirmed').should('be.visible');<br>    cy.url().should('include', '/order-confirmation');<br>  });<br>});<br></code></pre><br><br><h2>1.4 Tests API avec Postman</h2><br><br><h3>Présentation de Postman</h3><br><br>Postman est une plateforme complète pour le développement et test d'API :<br><li>Interface graphique intuitive</li><br><li>Collections et environnements</li><br><li>Tests automatisés avec scripts</li><br><li>Monitoring et documentation</li><br><br><h3>Fonctionnalités Clés</h3><br><li><strong>Collections</strong> : Organisation des requêtes</li><br><li><strong>Environments</strong> : Gestion des variables</li><br><li><strong>Tests Scripts</strong> : Validation automatisée</li><br><li><strong>Newman</strong> : Exécution en ligne de commande</li><br><br><h3>Exemple de Test Postman</h3><br><br><pre><code>javascript<br>// Test de création d'utilisateur<br>pm.test("User creation successful", function () {<br>    pm.response.to.have.status(201);<br>    <br>    const responseJson = pm.response.json();<br>    pm.expect(responseJson).to.have.property('id');<br>    pm.expect(responseJson.email).to.eql(pm.environment.get('user_email'));<br>    <br>    // Sauvegarder l'ID pour les tests suivants<br>    pm.environment.set('user_id', responseJson.id);<br>});<br><br>pm.test("Response time is acceptable", function () {<br>    pm.expect(pm.response.responseTime).to.be.below(2000);<br>});<br></code></pre><br><br><h2>1.5 Tests API avec RestAssured</h2><br><br><h3>Présentation de RestAssured</h3><br><br>RestAssured est une bibliothèque Java pour tester les services REST :<br><li>Syntaxe fluide et expressive</li><br><li>Validation JSON/XML intégrée</li><br><li>Support OAuth et authentification</li><br><li>Intégration JUnit/TestNG</li><br><br><h3>Avantages de RestAssured</h3><br><li>API intuitive (Given-When-Then)</li><br><li>Validation de schéma automatique</li><br><li>Gestion des cookies et sessions</li><br><li>Logging détaillé des requêtes/réponses</li><br><br><h3>Exemple de Test RestAssured</h3><br><br><pre><code>java<br>import static io.restassured.RestAssured.*;<br>import static org.hamcrest.Matchers.*;<br><br>public class UserApiTest {<br>    <br>    @Test<br>    public void testCreateUser() {<br>        given()<br>            .contentType("application/json")<br>            .body("{ \"name\": \"John Doe\", \"email\": \"john@example.com\" }")<br>        .when()<br>            .post("/api/users")<br>        .then()<br>            .statusCode(201)<br>            .body("name", equalTo("John Doe"))<br>            .body("email", equalTo("john@example.com"))<br>            .body("id", notNullValue())<br>            .time(lessThan(2000L));<br>    }<br>    <br>    @Test<br>    public void testGetUserById() {<br>        int userId = createTestUser();<br>        <br>        given()<br>            .pathParam("id", userId)<br>        .when()<br>            .get("/api/users/{id}")<br>        .then()<br>            .statusCode(200)<br>            .body("id", equalTo(userId))<br>            .body("name", notNullValue())<br>            .body("email", matchesPattern(".<em>@.</em>\\..*"));<br>    }<br>}<br></code></pre><br><br><h2>1.6 Stratégies de Test et Bonnes Pratiques</h2><br><br><h3>Pyramide des Tests</h3><br><br><pre><code><br>    E2E Tests (Peu)<br>   ↗              ↖<br>Integration Tests (Quelques)<br>↗                        ↖<br>Unit Tests (Beaucoup)<br></code></pre><br><br><h3>Bonnes Pratiques</h3><br><br><strong>Organisation des Tests</strong><br><li>Structure claire et cohérente</li><br><li>Nommage descriptif des tests</li><br><li>Groupement par fonctionnalité</li><br><li>Isolation des tests</li><br><br><strong>Données de Test</strong><br><li>Utilisation de fixtures</li><br><li>Nettoyage après chaque test</li><br><li>Données anonymisées</li><br><li>Environnements dédiés</li><br><br><strong>Maintenance</strong><br><li>Page Object Model pour UI</li><br><li>Factorisation du code commun</li><br><li>Gestion des sélecteurs robustes</li><br><li>Documentation des tests</li><br><br><h3>Intégration CI/CD</h3><br><br><strong>Configuration Pipeline</strong><br><pre><code>yaml<br>test-functional:<br>  stage: test<br>  script:<br>    - npm install<br>    - npm run test:api<br>    - npm run test:ui:headless<br>  artifacts:<br>    reports:<br>      junit: test-results.xml<br>    paths:<br>      - screenshots/<br>      - videos/<br></code></pre><br><br><strong>Parallélisation</strong><br><li>Exécution simultanée des tests</li><br><li>Distribution sur plusieurs agents</li><br><li>Optimisation des temps d'exécution</li><br><li>Gestion des ressources partagées</li><br><br><h1>2. Tests de Performance et de Charge</h1><br><br><h2>2.1 Concepts de Performance et Métriques Clés</h2><br><br><h3>Définitions Essentielles</h3><br><br><strong>Tests de Performance</strong><br><li>Évaluation des performances sous conditions normales</li><br><li>Mesure des temps de réponse et du débit</li><br><li>Identification des goulots d'étranglement</li><br><br><strong>Tests de Charge</strong><br><li>Validation sous charge utilisateur attendue</li><br><li>Vérification de la stabilité système</li><br><li>Mesure de la dégradation des performances</li><br><br><strong>Tests de Stress</strong><br><li>Évaluation au-delà des limites normales</li><br><li>Identification du point de rupture</li><br><li>Test de récupération après incident</li><br><br><h3>Métriques Clés de Performance</h3><br><br><strong>Temps de Réponse</strong><br><li>Temps moyen, médian, 95e percentile</li><br><li>Temps de première réponse (TTFB)</li><br><li>Temps de chargement complet</li><br><br><strong>Débit (Throughput)</strong><br><li>Requêtes par seconde (RPS)</li><br><li>Transactions par seconde (TPS)</li><br><li>Bande passante utilisée</li><br><br><strong>Utilisation des Ressources</strong><br><li>CPU, mémoire, disque, réseau</li><br><li>Connexions base de données</li><br><li>Files d'attente et pools de threads</li><br><br><strong>Métriques Utilisateur</strong><br><li>Taux d'erreur</li><br><li>Taux d'abandon</li><br><li>Satisfaction utilisateur (Apdex)</li><br><br><h2>2.2 Tests de Charge avec JMeter</h2><br><br><h3>Présentation d'Apache JMeter</h3><br><br>JMeter est un outil open-source pour les tests de performance :<br><li>Interface graphique intuitive</li><br><li>Support multi-protocoles (HTTP, JDBC, JMS, etc.)</li><br><li>Extensibilité via plugins</li><br><li>Rapports détaillés</li><br><br><h3>Architecture JMeter</h3><br><br><pre><code><br>Test Plan<br>├── Thread Groups (Utilisateurs virtuels)<br>├── Samplers (Requêtes)<br>├── Listeners (Collecte de résultats)<br>├── Timers (Délais)<br>└── Assertions (Validations)<br></code></pre><br><br><h3>Configuration d'un Test de Charge</h3><br><br><strong>1. Plan de Test Basique</strong><br><br><pre><code>xml<br><?xml version="1.0" encoding="UTF-8"?><br><jmeterTestPlan version="1.2"><br>  <hashTree><br>    <TestPlan testname="API Load Test"><br>      <elementProp name="TestPlan.arguments" elementType="Arguments"/><br>      <boolProp name="TestPlan.functional_mode">false</boolProp><br>      <boolProp name="TestPlan.serialize_threadgroups">false</boolProp><br>    </TestPlan><br>    <hashTree><br>      <ThreadGroup testname="Users"><br>        <stringProp name="ThreadGroup.num_threads">100</stringProp><br>        <stringProp name="ThreadGroup.ramp_time">60</stringProp><br>        <stringProp name="ThreadGroup.duration">300</stringProp><br>      </ThreadGroup><br>    </hashTree><br>  </hashTree><br></jmeterTestPlan><br></code></pre><br><br><strong>2. Exemple de Requête HTTP</strong><br><br><pre><code><br>HTTP Request Sampler:<br><li>Server: api.example.com</li><br><li>Port: 443</li><br><li>Protocol: https</li><br><li>Method: POST</li><br><li>Path: /api/users</li><br><li>Body: {"name": "Test User", "email": "test@example.com"}</li><br></code></pre><br><br><h3>Stratégies de Montée en Charge</h3><br><br><strong>Montée Progressive</strong><br><pre><code><br>Utilisateurs: 0 → 50 → 100 → 150 → 200<br>Durée: 0min → 15min → 30min → 45min → 60min<br></code></pre><br><br><strong>Test de Pic</strong><br><pre><code><br>Utilisateurs: 10 → 500 → 10<br>Durée: 0min → 5min → 10min<br></code></pre><br><br><strong>Test de Stabilité</strong><br><pre><code><br>Utilisateurs: 100 (constant)<br>Durée: 2 heures<br></code></pre><br><br><h2>2.3 Monitoring des Temps de Réponse</h2><br><br><h3>Outils de Monitoring</h3><br><br><strong>Application Performance Monitoring (APM)</strong><br><li>New Relic, Datadog, AppDynamics</li><br><li>Monitoring en temps réel</li><br><li>Alertes automatiques</li><br><li>Analyse des traces</li><br><br><strong>Monitoring Infrastructure</strong><br><li>Prometheus + Grafana</li><br><li>Métriques système et application</li><br><li>Dashboards personnalisés</li><br><li>Historique des performances</li><br><br><h3>Configuration Prometheus</h3><br><br><pre><code>yaml<br><h1>prometheus.yml</h1><br>global:<br>  scrape_interval: 15s<br><br>scrape_configs:<br>  - job_name: 'api-server'<br>    static_configs:<br>      - targets: ['localhost:8080']<br>    metrics_path: '/metrics'<br>    scrape_interval: 5s<br></code></pre><br><br><h3>Métriques Applicatives</h3><br><br><pre><code>javascript<br>// Exemple Node.js avec Prometheus<br>const promClient = require('prom-client');<br><br>const httpRequestDuration = new promClient.Histogram({<br>  name: 'http_request_duration_seconds',<br>  help: 'Duration of HTTP requests in seconds',<br>  labelNames: ['method', 'route', 'status_code'],<br>  buckets: [0.1, 0.3, 0.5, 0.7, 1, 3, 5, 7, 10]<br>});<br><br>app.use((req, res, next) => {<br>  const start = Date.now();<br>  <br>  res.on('finish', () => {<br>    const duration = (Date.now() - start) / 1000;<br>    httpRequestDuration<br>      .labels(req.method, req.route?.path || req.path, res.statusCode)<br>      .observe(duration);<br>  });<br>  <br>  next();<br>});<br></code></pre><br><br><h2>2.4 Analyse et Interprétation des Résultats</h2><br><br><h3>Lecture des Rapports JMeter</h3><br><br><strong>Métriques Principales</strong><br><li><strong>Average</strong> : Temps de réponse moyen</li><br><li><strong>Median</strong> : 50e percentile</li><br><li><strong>90% Line</strong> : 90e percentile</li><br><li><strong>95% Line</strong> : 95e percentile</li><br><li><strong>99% Line</strong> : 99e percentile</li><br><li><strong>Min/Max</strong> : Temps minimum et maximum</li><br><li><strong>Error %</strong> : Pourcentage d'erreurs</li><br><li><strong>Throughput</strong> : Débit (req/sec)</li><br><br><h3>Identification des Problèmes</h3><br><br><strong>Temps de Réponse Élevés</strong><br><pre><code><br>Causes possibles:<br><li>Requêtes base de données lentes</li><br><li>Goulots d'étranglement réseau</li><br><li>Traitement CPU intensif</li><br><li>Manque de mise en cache</li><br></code></pre><br><br><strong>Taux d'Erreur Élevé</strong><br><pre><code><br>Types d'erreurs:<br><li>5xx: Erreurs serveur</li><br><li>4xx: Erreurs client</li><br><li>Timeouts: Dépassement de délai</li><br><li>Connexions refusées</li><br></code></pre><br><br><h3>Optimisations Courantes</h3><br><br><strong>Base de Données</strong><br><li>Indexation appropriée</li><br><li>Optimisation des requêtes</li><br><li>Pool de connexions</li><br><li>Cache de requêtes</li><br><br><strong>Application</strong><br><li>Mise en cache (Redis, Memcached)</li><br><li>Optimisation des algorithmes</li><br><li>Lazy loading</li><br><li>Compression des réponses</li><br><br><strong>Infrastructure</strong><br><li>Load balancing</li><br><li>CDN pour les assets statiques</li><br><li>Scaling horizontal/vertical</li><br><li>Optimisation réseau</li><br><br><h2>2.5 Intégration dans le Pipeline CI/CD</h2><br><br><h3>Tests de Performance Automatisés</h3><br><br><pre><code>yaml<br><h1>.github/workflows/performance.yml</h1><br>name: Performance Tests<br><br>on:<br>  push:<br>    branches: [main]<br>  pull_request:<br>    branches: [main]<br><br>jobs:<br>  performance-test:<br>    runs-on: ubuntu-latest<br>    <br>    steps:<br>    - uses: actions/checkout@v2<br>    <br>    - name: Setup JMeter<br>      run: |<br>        wget https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-5.4.1.tgz<br>        tar -xzf apache-jmeter-5.4.1.tgz<br>        <br>    - name: Run Performance Tests<br>      run: |<br>        ./apache-jmeter-5.4.1/bin/jmeter -n -t tests/load-test.jmx -l results.jtl<br>        <br>    - name: Generate Report<br>      run: |<br>        ./apache-jmeter-5.4.1/bin/jmeter -g results.jtl -o report/<br>        <br>    - name: Upload Results<br>      uses: actions/upload-artifact@v2<br>      with:<br>        name: performance-report<br>        path: report/<br></code></pre><br><br><h3>Critères de Validation</h3><br><br><strong>Seuils de Performance</strong><br><pre><code>yaml<br>performance_thresholds:<br>  response_time_95th: 2000ms<br>  error_rate: 1%<br>  throughput_min: 100rps<br>  cpu_usage_max: 80%<br>  memory_usage_max: 85%<br></code></pre><br><br><strong>Alertes et Notifications</strong><br><li>Échec si seuils dépassés</li><br><li>Notifications Slack/Teams</li><br><li>Blocage du déploiement</li><br><li>Rapport automatique aux équipes</li><br><br><h3>Bonnes Pratiques</h3><br><br><strong>Environnement de Test</strong><br><li>Isolation des tests de performance</li><br><li>Données représentatives</li><br><li>Configuration similaire à la production</li><br><li>Nettoyage entre les tests</li><br><br><strong>Stratégie de Test</strong><br><li>Tests réguliers (nightly builds)</li><br><li>Tests sur les features critiques</li><br><li>Comparaison avec baseline</li><br><li>Tests de régression performance</li><br><br><h1>3. Tests de Sécurité Automatisés</h1><br><br><h2>3.1 Principes de Sécurité dans les Tests</h2><br><br><h3>Importance des Tests de Sécurité</h3><br><br>Les tests de sécurité automatisés sont essentiels pour :<br><li>Détecter les vulnérabilités tôt dans le cycle de développement</li><br><li>Réduire les coûts de correction des failles</li><br><li>Assurer la conformité aux standards de sécurité</li><br><li>Protéger les données sensibles et la réputation</li><br><br><h3>Types de Tests de Sécurité</h3><br><br><strong>Tests Statiques (SAST)</strong><br><li>Analyse du code source</li><br><li>Détection de patterns dangereux</li><br><li>Vérification des bonnes pratiques</li><br><li>Outils : SonarQube, Checkmarx, Veracode</li><br><br><strong>Tests Dynamiques (DAST)</strong><br><li>Analyse de l'application en fonctionnement</li><br><li>Tests de pénétration automatisés</li><br><li>Scan des vulnérabilités web</li><br><li>Outils : OWASP ZAP, Burp Suite, Nessus</li><br><br><strong>Tests Interactifs (IAST)</strong><br><li>Combinaison SAST + DAST</li><br><li>Analyse en temps réel</li><br><li>Contexte d'exécution précis</li><br><li>Outils : Contrast Security, Checkmarx IAST</li><br><br><h3>OWASP Top 10 - Vulnérabilités Critiques</h3><br><br>1. <strong>Injection</strong> - SQL, NoSQL, OS, LDAP<br>2. <strong>Broken Authentication</strong> - Gestion des sessions<br>3. <strong>Sensitive Data Exposure</strong> - Chiffrement insuffisant<br>4. <strong>XML External Entities (XXE)</strong> - Parseurs XML vulnérables<br>5. <strong>Broken Access Control</strong> - Contrôles d'autorisation<br>6. <strong>Security Misconfiguration</strong> - Configuration par défaut<br>7. <strong>Cross-Site Scripting (XSS)</strong> - Injection de scripts<br>8. <strong>Insecure Deserialization</strong> - Désérialisation non sécurisée<br>9. <strong>Using Components with Known Vulnerabilities</strong> - Dépendances<br>10. <strong>Insufficient Logging & Monitoring</strong> - Surveillance inadéquate<br><br><h2>3.2 Scan de Vulnérabilités avec OWASP ZAP</h2><br><br><h3>Présentation d'OWASP ZAP</h3><br><br>OWASP Zed Attack Proxy (ZAP) est un outil de test de sécurité :<br><li>Scanner de vulnérabilités web gratuit</li><br><li>Interface graphique et API</li><br><li>Proxy intercepteur</li><br><li>Extensible via add-ons</li><br><br><h3>Architecture ZAP</h3><br><br><pre><code><br>Browser → ZAP Proxy → Web Application<br>           ↓<br>    Vulnerability Scanner<br>           ↓<br>        Reports<br></code></pre><br><br><h3>Installation et Configuration</h3><br><br><strong>Installation Docker</strong><br><pre><code>bash<br><h1>Télécharger l'image ZAP</h1><br>docker pull owasp/zap2docker-stable<br><br><h1>Lancer ZAP en mode daemon</h1><br>docker run -u zap -p 8080:8080 -i owasp/zap2docker-stable zap.sh -daemon -host 0.0.0.0 -port 8080<br></code></pre><br><br><strong>Configuration de Base</strong><br><pre><code>bash<br><h1>Configuration du proxy</h1><br>export ZAP_PROXY=http://localhost:8080<br><br><h1>API Key pour l'authentification</h1><br>export ZAP_API_KEY=your-api-key-here<br></code></pre><br><br><h3>Scan Automatisé avec ZAP</h3><br><br><strong>Script de Scan Basique</strong><br><pre><code>bash<br>#!/bin/bash<br><br>TARGET_URL="http://localhost:3000"<br>ZAP_API="http://localhost:8080"<br><br><h1>Démarrer le spider pour découvrir les URLs</h1><br>curl "$ZAP_API/JSON/spider/action/scan/?url=$TARGET_URL"<br><br><h1>Attendre la fin du spider</h1><br>while [ $(curl -s "$ZAP_API/JSON/spider/view/status/" | jq -r '.status') != "100" ]; do<br>  echo "Spider en cours..."<br>  sleep 5<br>done<br><br><h1>Lancer le scan actif</h1><br>curl "$ZAP_API/JSON/ascan/action/scan/?url=$TARGET_URL"<br><br><h1>Attendre la fin du scan</h1><br>while [ $(curl -s "$ZAP_API/JSON/ascan/view/status/" | jq -r '.status') != "100" ]; do<br>  echo "Scan actif en cours..."<br>  sleep 10<br>done<br><br><h1>Générer le rapport</h1><br>curl "$ZAP_API/OTHER/core/other/htmlreport/" > security-report.html<br></code></pre><br><br><h3>Intégration dans les Tests</h3><br><br><strong>Test Selenium + ZAP</strong><br><pre><code>javascript<br>const { Builder, By } = require('selenium-webdriver');<br>const proxy = require('selenium-webdriver/proxy');<br><br>describe('Security Tests', () => {<br>  let driver;<br>  <br>  beforeAll(async () => {<br>    // Configuration du proxy ZAP<br>    const zapProxy = proxy.manual({<br>      http: 'localhost:8080',<br>      https: 'localhost:8080'<br>    });<br>    <br>    driver = await new Builder()<br>      .forBrowser('chrome')<br>      .setProxy(zapProxy)<br>      .build();<br>  });<br>  <br>  it('should perform authenticated scan', async () => {<br>    // Navigation authentifiée<br>    await driver.get('http://localhost:3000/login');<br>    await driver.findElement(By.id('username')).sendKeys('testuser');<br>    await driver.findElement(By.id('password')).sendKeys('password');<br>    await driver.findElement(By.css('button[type="submit"]')).click();<br>    <br>    // Navigation dans l'application<br>    await driver.get('http://localhost:3000/dashboard');<br>    await driver.get('http://localhost:3000/profile');<br>    await driver.get('http://localhost:3000/settings');<br>    <br>    // ZAP enregistre automatiquement toutes les requêtes<br>  });<br>});<br></code></pre><br><br><h2>3.3 Analyse des Dépendances avec Snyk</h2><br><br><h3>Présentation de Snyk</h3><br><br>Snyk est une plateforme de sécurité pour les développeurs :<br><li>Scan des dépendances open source</li><br><li>Détection des vulnérabilités connues</li><br><li>Suggestions de correction automatiques</li><br><li>Intégration CI/CD native</li><br><br><h3>Types de Scans Snyk</h3><br><br><strong>Snyk Open Source</strong><br><li>Vulnérabilités dans les dépendances</li><br><li>Licences problématiques</li><br><li>Suggestions de mise à jour</li><br><br><strong>Snyk Code</strong><br><li>Analyse statique du code</li><br><li>Détection de failles de sécurité</li><br><li>Recommandations de correction</li><br><br><strong>Snyk Container</strong><br><li>Scan des images Docker</li><br><li>Vulnérabilités du système de base</li><br><li>Optimisation des images</li><br><br><strong>Snyk Infrastructure as Code</strong><br><li>Scan des fichiers Terraform, Kubernetes</li><br><li>Détection de mauvaises configurations</li><br><li>Bonnes pratiques de sécurité</li><br><br><h3>Installation et Utilisation</h3><br><br><strong>Installation CLI</strong><br><pre><code>bash<br><h1>Installation via npm</h1><br>npm install -g snyk<br><br><h1>Authentification</h1><br>snyk auth<br><br><h1>Scan du projet</h1><br>snyk test<br><br><h1>Scan avec rapport JSON</h1><br>snyk test --json > security-report.json<br></code></pre><br><br><h3>Intégration dans le Pipeline</h3><br><br><strong>GitHub Actions avec Snyk</strong><br><pre><code>yaml<br>name: Security Scan<br><br>on: [push, pull_request]<br><br>jobs:<br>  security:<br>    runs-on: ubuntu-latest<br>    <br>    steps:<br>    - uses: actions/checkout@v2<br>    <br>    - name: Setup Node.js<br>      uses: actions/setup-node@v2<br>      with:<br>        node-version: '16'<br>        <br>    - name: Install dependencies<br>      run: npm ci<br>      <br>    - name: Run Snyk to check for vulnerabilities<br>      uses: snyk/actions/node@master<br>      env:<br>        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}<br>      with:<br>        args: --severity-threshold=high<br>        <br>    - name: Upload result to GitHub Code Scanning<br>      uses: github/codeql-action/upload-sarif@v1<br>      with:<br>        sarif_file: snyk.sarif<br></code></pre><br><br><h3>Configuration Avancée</h3><br><br><strong>Fichier .snyk</strong><br><pre><code>yaml<br><h1>Ignorer certaines vulnérabilités temporairement</h1><br>ignore:<br>  SNYK-JS-LODASH-567746:<br>    - '*':<br>        reason: 'Pas de fix disponible, risque acceptable'<br>        expires: '2024-12-31T23:59:59.999Z'<br><br><h1>Patches automatiques</h1><br>patches:<br>  SNYK-JS-MINIMIST-559764:<br>    - tap > nyc > minimist:<br>        patched: '2021-03-15T10:00:00.000Z'<br><br><h1>Exclusions de chemins</h1><br>exclude:<br>  global:<br>    - test/<em></em><br>    - docs/<em></em><br></code></pre><br><br><h2>3.4 Intégration dans le Pipeline CI/CD</h2><br><br><h3>Pipeline de Sécurité Complet</h3><br><br><pre><code>yaml<br><h1>.github/workflows/security.yml</h1><br>name: Security Pipeline<br><br>on:<br>  push:<br>    branches: [main, develop]<br>  pull_request:<br>    branches: [main]<br><br>jobs:<br>  dependency-scan:<br>    name: Dependency Vulnerability Scan<br>    runs-on: ubuntu-latest<br>    <br>    steps:<br>    - uses: actions/checkout@v2<br>    <br>    - name: Snyk Dependency Scan<br>      uses: snyk/actions/node@master<br>      env:<br>        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}<br>      with:<br>        args: --severity-threshold=medium<br>        <br>  code-scan:<br>    name: Static Code Analysis<br>    runs-on: ubuntu-latest<br>    <br>    steps:<br>    - uses: actions/checkout@v2<br>    <br>    - name: SonarCloud Scan<br>      uses: SonarSource/sonarcloud-github-action@master<br>      env:<br>        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}<br>        SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}<br>        <br>  dynamic-scan:<br>    name: Dynamic Security Testing<br>    runs-on: ubuntu-latest<br>    needs: [dependency-scan, code-scan]<br>    <br>    steps:<br>    - uses: actions/checkout@v2<br>    <br>    - name: Start Application<br>      run: |<br>        docker-compose up -d<br>        sleep 30<br>        <br>    - name: OWASP ZAP Scan<br>      uses: zaproxy/action-full-scan@v0.4.0<br>      with:<br>        target: 'http://localhost:3000'<br>        rules_file_name: '.zap/rules.tsv'<br>        cmd_options: '-a'<br>        <br>    - name: Stop Application<br>      run: docker-compose down<br></code></pre><br><br><h3>Gestion des Résultats</h3><br><br><strong>Seuils de Sécurité</strong><br><pre><code>yaml<br>security_thresholds:<br>  critical_vulnerabilities: 0<br>  high_vulnerabilities: 2<br>  medium_vulnerabilities: 10<br>  low_vulnerabilities: 50<br>  <br>quality_gates:<br>  block_deployment_on_critical: true<br>  require_approval_on_high: true<br>  notify_team_on_medium: true<br></code></pre><br><br><strong>Rapports et Notifications</strong><br><pre><code>javascript<br>// Exemple de notification Slack<br>const sendSecurityAlert = async (vulnerabilities) => {<br>  const criticalCount = vulnerabilities.filter(v => v.severity === 'critical').length;<br>  const highCount = vulnerabilities.filter(v => v.severity === 'high').length;<br>  <br>  if (criticalCount > 0 || highCount > 5) {<br>    await slack.chat.postMessage({<br>      channel: '#security-alerts',<br>      text: <code>🚨 Vulnérabilités détectées: ${criticalCount} critiques, ${highCount} élevées</code>,<br>      attachments: [{<br>        color: 'danger',<br>        fields: vulnerabilities.slice(0, 5).map(v => ({<br>          title: v.title,<br>          value: <code>Sévérité: ${v.severity}\nCVE: ${v.cve}</code>,<br>          short: true<br>        }))<br>      }]<br>    });<br>  }<br>};<br></code></pre><br><br><h2>3.5 Bonnes Pratiques de Sécurité</h2><br><br><h3>Shift-Left Security</h3><br><br><strong>Intégration Précoce</strong><br><li>Tests de sécurité dès le développement</li><br><li>Formation des développeurs</li><br><li>Outils intégrés dans l'IDE</li><br><li>Revues de code sécurisées</li><br><br><strong>Automatisation Complète</strong><br><li>Scans à chaque commit</li><br><li>Validation des pull requests</li><br><li>Déploiement conditionnel</li><br><li>Monitoring continu</li><br><br><h3>Gestion des Secrets</h3><br><br><strong>Bonnes Pratiques</strong><br><pre><code>yaml<br><h1>Mauvais - secrets en dur</h1><br>database_url: "postgresql://user:password@localhost/db"<br><br><h1>Bon - utilisation de variables d'environnement</h1><br>database_url: "${DATABASE_URL}"<br></code></pre><br><br><strong>Outils de Gestion</strong><br><li>HashiCorp Vault</li><br><li>AWS Secrets Manager</li><br><li>Azure Key Vault</li><br><li>Kubernetes Secrets</li><br><br><h3>Monitoring et Réponse</h3><br><br><strong>Détection d'Intrusion</strong><br><li>Logs d'accès anormaux</li><br><li>Tentatives d'authentification</li><br><li>Patterns d'attaque connus</li><br><li>Alertes en temps réel</li><br><br><strong>Plan de Réponse</strong><br>1. Détection automatique<br>2. Isolation des systèmes<br>3. Analyse forensique<br>4. Correction et patch<br>5. Post-mortem et amélioration<br><br><h1>4. Environnements de Test Cloud</h1><br><br><h2>4.1 Avantages des Environnements Cloud</h2><br><br><h3>Bénéfices Principaux</h3><br><br><strong>Scalabilité Élastique</strong><br><li>Adaptation automatique à la charge</li><br><li>Provisioning rapide des ressources</li><br><li>Tests de montée en charge réalistes</li><br><li>Optimisation des coûts</li><br><br><strong>Disponibilité Globale</strong><br><li>Tests multi-régions</li><br><li>Simulation de latence réseau</li><br><li>Validation de la géo-réplication</li><br><li>Tests de disaster recovery</li><br><br><strong>Diversité des Environnements</strong><br><li>Multiples OS et navigateurs</li><br><li>Versions différentes des runtime</li><br><li>Configurations matérielles variées</li><br><li>Tests de compatibilité étendus</li><br><br><h3>Comparaison Cloud vs On-Premise</h3><br><br>| Aspect | Cloud | On-Premise |<br>|--------|-------|------------|<br>| <strong>Coût initial</strong> | Faible | Élevé |<br>| <strong>Maintenance</strong> | Gérée par le provider | À charge de l'équipe |<br>| <strong>Scalabilité</strong> | Élastique | Limitée par le matériel |<br>| <strong>Sécurité</strong> | Partagée | Contrôle total |<br>| <strong>Latence</strong> | Variable | Prévisible |<br>| <strong>Compliance</strong> | Dépend du provider | Contrôle total |<br><br><h2>4.2 Plateformes de Test Cloud</h2><br><br><h3>BrowserStack</h3><br><br><strong>Fonctionnalités Clés</strong><br><li>3000+ combinaisons navigateur/OS</li><br><li>Tests en temps réel et automatisés</li><br><li>Debugging interactif</li><br><li>Intégration CI/CD native</li><br><br><strong>Exemple d'Intégration</strong><br><pre><code>javascript<br>// Configuration BrowserStack<br>const capabilities = {<br>  'browserName': 'Chrome',<br>  'browserVersion': 'latest',<br>  'os': 'Windows',<br>  'osVersion': '10',<br>  'buildName': 'CI Build #123',<br>  'sessionName': 'Login Test',<br>  'local': 'false'<br>};<br><br>const driver = new webdriver.Builder()<br>  .usingServer('https://hub-cloud.browserstack.com/wd/hub')<br>  .withCapabilities(capabilities)<br>  .build();<br></code></pre><br><br><h3>Sauce Labs</h3><br><br><strong>Avantages Spécifiques</strong><br><li>Tests sur appareils mobiles réels</li><br><li>Analytics et insights détaillés</li><br><li>Tests de performance intégrés</li><br><li>Support des frameworks populaires</li><br><br><strong>Configuration CI/CD</strong><br><pre><code>yaml<br><h1>GitHub Actions avec Sauce Labs</h1><br><li>name: Run Tests on Sauce Labs</li><br>  env:<br>    SAUCE_USERNAME: ${{ secrets.SAUCE_USERNAME }}<br>    SAUCE_ACCESS_KEY: ${{ secrets.SAUCE_ACCESS_KEY }}<br>  run: |<br>    npm test -- --sauce<br></code></pre><br><br><h3>AWS Device Farm</h3><br><br><strong>Spécificités AWS</strong><br><li>Tests sur appareils mobiles physiques</li><br><li>Intégration native avec AWS</li><br><li>Tests automatisés et exploratoires</li><br><li>Rapports détaillés avec captures</li><br><br><h3>Kubernetes pour Tests</h3><br><br><strong>Avantages de K8s</strong><br><li>Orchestration des environnements de test</li><br><li>Isolation des tests</li><br><li>Scaling automatique</li><br><li>Gestion des ressources</li><br><br><strong>Exemple de Déploiement</strong><br><pre><code>yaml<br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: test-environment<br>spec:<br>  replicas: 3<br>  selector:<br>    matchLabels:<br>      app: test-app<br>  template:<br>    metadata:<br>      labels:<br>        app: test-app<br>    spec:<br>      containers:<br>      - name: app<br>        image: myapp:test<br>        ports:<br>        - containerPort: 3000<br>        env:<br>        - name: NODE_ENV<br>          value: "test"<br>        - name: DATABASE_URL<br>          valueFrom:<br>            secretKeyRef:<br>              name: db-secret<br>              key: url<br></code></pre><br><br><h2>4.3 Configuration et Orchestration</h2><br><br><h3>Infrastructure as Code (IaC)</h3><br><br><strong>Terraform pour AWS</strong><br><pre><code>hcl<br><h1>Environnement de test automatisé</h1><br>resource "aws_instance" "test_server" {<br>  count         = var.test_instances<br>  ami           = "ami-0c55b159cbfafe1d0"<br>  instance_type = "t3.medium"<br>  <br>  tags = {<br>    Name        = "test-server-${count.index}"<br>    Environment = "testing"<br>    Purpose     = "automated-testing"<br>  }<br>  <br>  user_data = <<-EOF<br>    #!/bin/bash<br>    docker run -d -p 80:3000 myapp:${var.app_version}<br>    docker run -d -p 8080:8080 owasp/zap2docker-stable<br>  EOF<br>}<br><br>resource "aws_lb" "test_lb" {<br>  name               = "test-load-balancer"<br>  internal           = false<br>  load_balancer_type = "application"<br>  <br>  dynamic "subnet_mapping" {<br>    for_each = aws_instance.test_server<br>    content {<br>      subnet_id = subnet_mapping.value.subnet_id<br>    }<br>  }<br>}<br></code></pre><br><br><strong>Docker Compose pour Environnements Locaux</strong><br><pre><code>yaml<br>version: '3.8'<br><br>services:<br>  app:<br>    build: .<br>    ports:<br>      - "3000:3000"<br>    environment:<br>      - NODE_ENV=test<br>      - DATABASE_URL=postgresql://test:test@db:5432/testdb<br>    depends_on:<br>      - db<br>      - redis<br>    <br>  db:<br>    image: postgres:13<br>    environment:<br>      - POSTGRES_DB=testdb<br>      - POSTGRES_USER=test<br>      - POSTGRES_PASSWORD=test<br>    volumes:<br>      - test_db_data:/var/lib/postgresql/data<br>    <br>  redis:<br>    image: redis:6-alpine<br>    <br>  selenium-hub:<br>    image: selenium/hub:4.0.0<br>    ports:<br>      - "4444:4444"<br>    <br>  selenium-chrome:<br>    image: selenium/node-chrome:4.0.0<br>    depends_on:<br>      - selenium-hub<br>    environment:<br>      - HUB_HOST=selenium-hub<br>    <br>  zap:<br>    image: owasp/zap2docker-stable<br>    ports:<br>      - "8080:8080"<br>    command: zap.sh -daemon -host 0.0.0.0 -port 8080<br><br>volumes:<br>  test_db_data:<br></code></pre><br><br><h3>Gestion des Données de Test</h3><br><br><strong>Stratégies de Données</strong><br><pre><code>javascript<br>// Factory pour génération de données<br>class TestDataFactory {<br>  static createUser(overrides = {}) {<br>    return {<br>      id: faker.datatype.uuid(),<br>      name: faker.name.findName(),<br>      email: faker.internet.email(),<br>      createdAt: faker.date.recent(),<br>      ...overrides<br>    };<br>  }<br>  <br>  static createProduct(overrides = {}) {<br>    return {<br>      id: faker.datatype.uuid(),<br>      name: faker.commerce.productName(),<br>      price: faker.commerce.price(),<br>      category: faker.commerce.department(),<br>      ...overrides<br>    };<br>  }<br>}<br><br>// Seeding de base de données<br>const seedDatabase = async () => {<br>  await db.users.deleteMany({});<br>  await db.products.deleteMany({});<br>  <br>  const users = Array.from({ length: 100 }, () => TestDataFactory.createUser());<br>  const products = Array.from({ length: 50 }, () => TestDataFactory.createProduct());<br>  <br>  await db.users.insertMany(users);<br>  await db.products.insertMany(products);<br>};<br></code></pre><br><br><h2>4.4 Optimisation des Coûts et Performances</h2><br><br><h3>Stratégies d'Optimisation des Coûts</h3><br><br><strong>Scheduling Intelligent</strong><br><pre><code>yaml<br><h1>Tests programmés pendant les heures creuses</h1><br>schedule:<br>  - cron: '0 2 <em> </em> *'  # 2h du matin UTC<br>    branches: [main]<br>    <br>  - cron: '0 14 <em> </em> 1-5'  # 14h en semaine<br>    branches: [develop]<br></code></pre><br><br><strong>Auto-scaling Basé sur la Charge</strong><br><pre><code>yaml<br>apiVersion: autoscaling/v2<br>kind: HorizontalPodAutoscaler<br>metadata:<br>  name: test-app-hpa<br>spec:<br>  scaleTargetRef:<br>    apiVersion: apps/v1<br>    kind: Deployment<br>    name: test-app<br>  minReplicas: 1<br>  maxReplicas: 10<br>  metrics:<br>  - type: Resource<br>    resource:<br>      name: cpu<br>      target:<br>        type: Utilization<br>        averageUtilization: 70<br></code></pre><br><br><strong>Spot Instances pour Tests</strong><br><pre><code>hcl<br>resource "aws_spot_instance_request" "test_spot" {<br>  ami           = "ami-0c55b159cbfafe1d0"<br>  instance_type = "c5.large"<br>  spot_price    = "0.05"<br>  <br>  tags = {<br>    Name = "test-spot-instance"<br>  }<br>  <br>  # Arrêt automatique après 2h<br>  user_data = <<-EOF<br>    #!/bin/bash<br>    echo "sudo shutdown -h +120" | at now<br>  EOF<br>}<br></code></pre><br><br><h3>Optimisation des Performances</h3><br><br><strong>Mise en Cache des Artefacts</strong><br><pre><code>yaml<br><h1>GitHub Actions avec cache</h1><br><li>name: Cache Dependencies</li><br>  uses: actions/cache@v2<br>  with:<br>    path: |<br>      ~/.npm<br>      node_modules<br>    key: ${{ runner.os }}-node-${{ hashFiles('<em></em>/package-lock.json') }}<br>    <br><li>name: Cache Docker Layers</li><br>  uses: actions/cache@v2<br>  with:<br>    path: /tmp/.buildx-cache<br>    key: ${{ runner.os }}-buildx-${{ github.sha }}<br>    restore-keys: |<br>      ${{ runner.os }}-buildx-<br></code></pre><br><br><strong>Parallélisation des Tests</strong><br><pre><code>javascript<br>// Configuration Jest pour tests parallèles<br>module.exports = {<br>  maxWorkers: '50%',<br>  testPathIgnorePatterns: ['/node_modules/', '/build/'],<br>  setupFilesAfterEnv: ['<rootDir>/src/setupTests.js'],<br>  <br>  // Groupement des tests par type<br>  projects: [<br>    {<br>      displayName: 'unit',<br>      testMatch: ['<rootDir>/src/<em></em>/*.test.js']<br>    },<br>    {<br>      displayName: 'integration',<br>      testMatch: ['<rootDir>/tests/integration/<em></em>/*.test.js']<br>    },<br>    {<br>      displayName: 'e2e',<br>      testMatch: ['<rootDir>/tests/e2e/<em></em>/*.test.js'],<br>      maxWorkers: 1  // Tests E2E séquentiels<br>    }<br>  ]<br>};<br></code></pre><br><br><h2>4.5 Bonnes Pratiques de Déploiement</h2><br><br><h3>Environnements Éphémères</h3><br><br><strong>Pull Request Environments</strong><br><pre><code>yaml<br>name: PR Environment<br><br>on:<br>  pull_request:<br>    types: [opened, synchronize]<br><br>jobs:<br>  deploy-pr-env:<br>    runs-on: ubuntu-latest<br>    <br>    steps:<br>    - name: Deploy to PR Environment<br>      run: |<br>        # Créer un environnement unique pour la PR<br>        ENV_NAME="pr-${{ github.event.number }}"<br>        <br>        # Déployer l'application<br>        kubectl create namespace $ENV_NAME<br>        kubectl apply -f k8s/ -n $ENV_NAME<br>        <br>        # Configurer l'URL unique<br>        echo "Environment URL: https://$ENV_NAME.test.example.com"<br>        <br>    - name: Run Tests Against PR Environment<br>      run: |<br>        export TEST_URL="https://pr-${{ github.event.number }}.test.example.com"<br>        npm run test:e2e<br></code></pre><br><br><h3>Blue-Green Deployment pour Tests</h3><br><br><pre><code>yaml<br><h1>Configuration Blue-Green</h1><br>apiVersion: v1<br>kind: Service<br>metadata:<br>  name: app-service<br>spec:<br>  selector:<br>    app: myapp<br>    version: blue  # Bascule entre blue et green<br>  ports:<br>  - port: 80<br>    targetPort: 3000<br><br>---<br><h1>Déploiement Green (nouvelle version)</h1><br>apiVersion: apps/v1<br>kind: Deployment<br>metadata:<br>  name: app-green<br>spec:<br>  replicas: 3<br>  selector:<br>    matchLabels:<br>      app: myapp<br>      version: green<br>  template:<br>    metadata:<br>      labels:<br>        app: myapp<br>        version: green<br>    spec:<br>      containers:<br>      - name: app<br>        image: myapp:v2.0.0<br></code></pre><br><br><h3>Monitoring et Observabilité</h3><br><br><strong>Métriques Personnalisées</strong><br><pre><code>javascript<br>// Métriques de test avec Prometheus<br>const promClient = require('prom-client');<br><br>const testExecutionTime = new promClient.Histogram({<br>  name: 'test_execution_duration_seconds',<br>  help: 'Time spent executing tests',<br>  labelNames: ['test_suite', 'environment', 'status']<br>});<br><br>const testResults = new promClient.Counter({<br>  name: 'test_results_total',<br>  help: 'Total number of test results',<br>  labelNames: ['test_suite', 'status', 'environment']<br>});<br><br>// Utilisation dans les tests<br>const startTime = Date.now();<br>try {<br>  await runTestSuite();<br>  testResults.labels('e2e', 'passed', 'staging').inc();<br>} catch (error) {<br>  testResults.labels('e2e', 'failed', 'staging').inc();<br>} finally {<br>  const duration = (Date.now() - startTime) / 1000;<br>  testExecutionTime.labels('e2e', 'staging', 'completed').observe(duration);<br>}<br></code></pre><br><br><strong>Dashboards Grafana</strong><br><pre><code>json<br>{<br>  "dashboard": {<br>    "title": "Test Environment Monitoring",<br>    "panels": [<br>      {<br>        "title": "Test Success Rate",<br>        "type": "stat",<br>        "targets": [<br>          {<br>            "expr": "rate(test_results_total{status=\"passed\"}[5m]) / rate(test_results_total[5m]) * 100"<br>          }<br>        ]<br>      },<br>      {<br>        "title": "Test Execution Time",<br>        "type": "graph",<br>        "targets": [<br>          {<br>            "expr": "histogram_quantile(0.95, test_execution_duration_seconds_bucket)"<br>          }<br>        ]<br>      }<br>    ]<br>  }<br>}<br></code></pre><br><br><h1>Module 3 - Tests Fonctionnels et Non-Fonctionnels</h1><br><br><h2>Vue d'ensemble</h2><br><br>Ce module couvre les aspects essentiels des tests automatisés dans un environnement CI/CD, en se concentrant sur les tests fonctionnels et non-fonctionnels. Les apprenants découvriront les outils et techniques pour automatiser les tests UI, API, de performance et de sécurité.<br><br><h2>Objectifs pédagogiques</h2><br><br>À l'issue de ce module, les apprenants seront capables de :<br><br><li>Implémenter des tests UI automatisés avec Selenium et Cypress</li><br><li>Créer des tests API robustes avec Postman et RestAssured</li><br><li>Configurer des tests de performance et de charge avec JMeter</li><br><li>Mettre en place des tests de sécurité automatisés avec OWASP ZAP</li><br><li>Utiliser les environnements de test cloud pour l'optimisation</li><br><li>Intégrer ces tests dans un pipeline CI/CD complet</li><br><br><h2>Structure du contenu</h2><br><br>1. <strong>Tests Fonctionnels Automatisés</strong> (12 slides)<br>   - Introduction aux tests fonctionnels<br>   - Tests UI avec Selenium et Cypress<br>   - Tests API avec Postman et RestAssured<br>   - Stratégies de test et bonnes pratiques<br><br>2. <strong>Tests de Performance et de Charge</strong> (10 slides)<br>   - Concepts de performance et métriques clés<br>   - Tests de charge avec JMeter<br>   - Monitoring des temps de réponse<br>   - Analyse et interprétation des résultats<br><br>3. <strong>Tests de Sécurité Automatisés</strong> (8 slides)<br>   - Principes de sécurité dans les tests<br>   - Scan de vulnérabilités avec OWASP ZAP<br>   - Analyse des dépendances avec Snyk<br>   - Intégration dans le pipeline CI/CD<br><br>4. <strong>Environnements de Test Cloud</strong> (5 slides)<br>   - Avantages des environnements cloud<br>   - Configuration et orchestration<br>   - Optimisation des coûts et performances<br>   - Bonnes pratiques de déploiement<br><br><h2>Durée estimée</h2><br><br><li><strong>Théorie</strong> : 2h30</li><br><li><strong>Exercices pratiques</strong> : 4h</li><br><li><strong>QCM et synthèse</strong> : 30min</li><br><li><strong>Total</strong> : 7h (1,5 jour)</li><br><br><h2>Prérequis</h2><br><br><li>Connaissances de base en développement web</li><br><li>Familiarité avec les concepts CI/CD (Module 1)</li><br><li>Compréhension des tests automatisés</li><br><li>Accès aux outils : Selenium, Cypress, JMeter, OWASP ZAP</li><br><br><h2>Ressources nécessaires</h2><br><br><li>Environnement de développement configuré</li><br><li>Applications de test (fournie)</li><br><li>Accès internet pour les outils cloud</li><br><li>Comptes sur les plateformes de test (optionnel)</li><br><br>\newpage<br><br><h1>Exercices Pratiques</h1><br><br><h1>Exercices Pratiques - Module 3</h1><br><br><h2>Vue d'ensemble</h2><br><br>Ce module contient 6 exercices pratiques couvrant les tests fonctionnels et non-fonctionnels :<br><br>1. <strong>Tests UI avec Selenium et Cypress</strong> - Automatisation des tests d'interface utilisateur<br>2. <strong>Tests API avec Postman et RestAssured</strong> - Validation des services web<br>3. <strong>Simulation de charge avec JMeter</strong> - Tests de performance et de montée en charge<br>4. <strong>Monitoring des temps de réponse</strong> - Surveillance et métriques de performance<br>5. <strong>Scan de vulnérabilités avec OWASP ZAP</strong> - Tests de sécurité automatisés<br>6. <strong>Analyse des dépendances avec Snyk</strong> - Détection de vulnérabilités dans les dépendances<br><br><h2>Prérequis Techniques</h2><br><br><li>Node.js 16+ et npm</li><br><li>Java 11+ (pour RestAssured et JMeter)</li><br><li>Docker et Docker Compose</li><br><li>Git</li><br><li>Navigateur Chrome/Firefox</li><br><br><h2>Installation des Outils</h2><br><br><pre><code>bash<br><h1>Installation des dépendances Node.js</h1><br>npm install -g @cypress/cli selenium-webdriver<br><br><h1>Installation de JMeter</h1><br>wget https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-5.4.1.tgz<br>tar -xzf apache-jmeter-5.4.1.tgz<br><br><h1>Installation de Snyk CLI</h1><br>npm install -g snyk<br><br><h1>Images Docker nécessaires</h1><br>docker pull owasp/zap2docker-stable<br>docker pull selenium/standalone-chrome<br></code></pre><br><br><h2>Structure des Exercices</h2><br><br>Chaque exercice suit la même structure :<br><li><code>README.md</code> - Instructions détaillées</li><br><li><code>ressources/</code> - Code de base et fichiers de configuration</li><br><li><code>solution/</code> - Solution complète avec explications</li><br><br><h2>Durée Estimée</h2><br><br><li><strong>Exercice 3.1</strong> : 45 minutes</li><br><li><strong>Exercice 3.2</strong> : 45 minutes  </li><br><li><strong>Exercice 3.3</strong> : 60 minutes</li><br><li><strong>Exercice 3.4</strong> : 30 minutes</li><br><li><strong>Exercice 3.5</strong> : 45 minutes</li><br><li><strong>Exercice 3.6</strong> : 30 minutes</li><br><br><strong>Total</strong> : 4h15 (avec pauses et discussions)<br><br><h2>Ordre Recommandé</h2><br><br>1. Commencer par les tests UI (3.1) pour établir les bases<br>2. Enchaîner avec les tests API (3.2) pour la complémentarité<br>3. Aborder les tests de performance (3.3 et 3.4) ensemble<br>4. Terminer par la sécurité (3.5 et 3.6) pour une approche complète<br><br>
</body>
</html>