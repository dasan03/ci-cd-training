<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Module 4 - Documentation et Monitoring</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 2cm; line-height: 1.6; }
        h1 { color: #2c3e50; border-bottom: 2px solid #3498db; }
        h2 { color: #34495e; margin-top: 2em; }
        h3 { color: #7f8c8d; }
        code { background: #f8f9fa; padding: 2px 4px; border-radius: 3px; }
        pre { background: #f8f9fa; padding: 1em; border-radius: 5px; overflow-x: auto; }
        li { margin: 0.5em 0; }
        @media print {
            body { margin: 1cm; }
            h1 { page-break-before: always; }
        }
    </style>
</head>
<body>
    <h1>Module 4 - Documentation et Monitoring</h1>
    <h1>Module 4 - Documentation et Monitoring</h1><br><br><h1>Module 4 : Documentation, reporting et monitoring des tests</h1><br><br><h2>Objectifs du module</h2><br><li>Rédiger une documentation claire et détaillée des tests</li><br><li>Générer des rapports de tests automatisés pour un suivi efficace</li><br><li>Mettre en place un monitoring des tests pour anticiper les régressions</li><br><br><h2>Durée</h2><br>2 heures (0,5 jour)<br><br><h2>Prérequis</h2><br><li>Outils de reporting : Allure Report, Extent Reports</li><br><li>Solutions de monitoring et alerting (Prometheus, Grafana, ELK Stack)</li><br><li>Systèmes de documentation collaboratifs (Confluence, Notion)</li><br><br><h2>Structure du module</h2><br><li><code>support-theorique/</code> - Contenu des cours et présentations</li><br><li><code>exercices/</code> - Exercices pratiques avec solutions</li><br><li><code>qcm/</code> - Questions d'évaluation intermédiaire</li><br><li><code>ressources/</code> - Fichiers de support et templates</li><br><br>\newpage<br><br><h1>Support Théorique</h1><br><br><h1>1. Documentation des Tests Automatisés</h1><br><br><h2>1.1 Importance de la Documentation</h2><br><br><h3>Pourquoi documenter les tests ?</h3><br><br>La documentation des tests automatisés est cruciale pour :<br><br><li><strong>Maintenabilité</strong> : Faciliter la compréhension et la modification des tests</li><br><li><strong>Collaboration</strong> : Permettre aux équipes de comprendre les tests existants</li><br><li><strong>Traçabilité</strong> : Lier les tests aux exigences métier</li><br><li><strong>Onboarding</strong> : Accélérer l'intégration de nouveaux développeurs</li><br><li><strong>Audit</strong> : Démontrer la couverture et la qualité des tests</li><br><br><h3>Impact sur la qualité</h3><br><br>Une bonne documentation :<br><li>Réduit le temps de maintenance des tests</li><br><li>Améliore la fiabilité des tests</li><br><li>Facilite la détection des tests obsolètes</li><br><li>Permet une meilleure couverture fonctionnelle</li><br><br><h2>1.2 Standards et Bonnes Pratiques</h2><br><br><h3>Niveaux de documentation</h3><br><br>1. <strong>Documentation du code</strong><br>   - Commentaires explicatifs<br>   - Annotations des méthodes de test<br>   - Description des données de test<br><br>2. <strong>Documentation fonctionnelle</strong><br>   - Scénarios de test détaillés<br>   - Cas d'usage couverts<br>   - Critères d'acceptation<br><br>3. <strong>Documentation technique</strong><br>   - Architecture des tests<br>   - Configuration des environnements<br>   - Procédures d'exécution<br><br><h3>Standards de nommage</h3><br><br><pre><code>javascript<br>// ❌ Mauvais nommage<br>test('test1', () => { ... });<br><br>// ✅ Bon nommage<br>test('should_create_user_when_valid_data_provided', () => { ... });<br>test('should_return_error_when_email_already_exists', () => { ... });<br></code></pre><br><br><h3>Structure des commentaires</h3><br><br><pre><code>javascript<br>/<em></em><br> * Test de création d'utilisateur avec données valides<br> * <br> * @description Vérifie que la création d'un utilisateur fonctionne<br> *              avec des données valides et retourne les bonnes informations<br> * @given Un utilisateur avec email et mot de passe valides<br> * @when L'utilisateur soumet le formulaire de création<br> * @then L'utilisateur est créé et un ID est retourné<br> * @requirement REQ-USER-001<br> */<br>test('should_create_user_when_valid_data_provided', async () => {<br>  // Arrange<br>  const userData = {<br>    email: 'test@example.com',<br>    password: 'SecurePass123!'<br>  };<br>  <br>  // Act<br>  const result = await userService.createUser(userData);<br>  <br>  // Assert<br>  expect(result.id).toBeDefined();<br>  expect(result.email).toBe(userData.email);<br>});<br></code></pre><br><br><h2>1.3 Documentation du Code de Test</h2><br><br><h3>Annotations et métadonnées</h3><br><br><pre><code>python<br>import pytest<br><br>@pytest.mark.smoke<br>@pytest.mark.user_management<br>@pytest.mark.requirement("REQ-USER-001")<br>def test_user_creation_with_valid_data():<br>    """<br>    Test la création d'un utilisateur avec des données valides.<br>    <br>    Ce test vérifie que :<br>    - L'utilisateur est créé avec succès<br>    - Les données sont correctement sauvegardées<br>    - Un ID unique est généré<br>    <br>    Données de test :<br>    - Email : test@example.com<br>    - Mot de passe : SecurePass123!<br>    <br>    Résultat attendu :<br>    - Code de retour : 201<br>    - Objet utilisateur avec ID généré<br>    """<br>    # Test implementation<br>    pass<br></code></pre><br><br><h3>Documentation des données de test</h3><br><br><pre><code>yaml<br><h1>test-data.yml</h1><br>user_creation_scenarios:<br>  valid_user:<br>    description: "Utilisateur avec données valides"<br>    email: "test@example.com"<br>    password: "SecurePass123!"<br>    expected_result: "success"<br>    <br>  invalid_email:<br>    description: "Email invalide"<br>    email: "invalid-email"<br>    password: "SecurePass123!"<br>    expected_result: "validation_error"<br>    expected_message: "Format d'email invalide"<br></code></pre><br><br><h2>1.4 Documentation des Résultats</h2><br><br><h3>Rapports de test structurés</h3><br><br>Les rapports doivent inclure :<br><br>1. <strong>Résumé exécutif</strong><br>   - Nombre de tests exécutés<br>   - Taux de réussite<br>   - Temps d'exécution total<br><br>2. <strong>Détails par catégorie</strong><br>   - Tests fonctionnels<br>   - Tests de régression<br>   - Tests de performance<br><br>3. <strong>Analyse des échecs</strong><br>   - Causes identifiées<br>   - Impact sur le système<br>   - Actions correctives<br><br><h3>Exemple de structure de rapport</h3><br><br><pre><code>json<br>{<br>  "test_execution": {<br>    "timestamp": "2024-01-15T10:30:00Z",<br>    "environment": "staging",<br>    "total_tests": 150,<br>    "passed": 142,<br>    "failed": 6,<br>    "skipped": 2,<br>    "duration": "00:12:34"<br>  },<br>  "categories": {<br>    "unit_tests": {<br>      "total": 80,<br>      "passed": 78,<br>      "failed": 2<br>    },<br>    "integration_tests": {<br>      "total": 45,<br>      "passed": 42,<br>      "failed": 3<br>    },<br>    "e2e_tests": {<br>      "total": 25,<br>      "passed": 22,<br>      "failed": 1,<br>      "skipped": 2<br>    }<br>  },<br>  "failures": [<br>    {<br>      "test_name": "test_user_login_with_invalid_credentials",<br>      "category": "integration",<br>      "error_message": "Expected 401, got 500",<br>      "stack_trace": "...",<br>      "screenshot": "path/to/screenshot.png"<br>    }<br>  ]<br>}<br></code></pre><br><br><h2>1.5 Outils de Documentation</h2><br><br><h3>Générateurs de documentation</h3><br><br>1. <strong>JSDoc</strong> (JavaScript)<br>   - Génération automatique de documentation<br>   - Intégration avec les IDE<br>   - Support des annotations personnalisées<br><br>2. <strong>Sphinx</strong> (Python)<br>   - Documentation riche en format HTML<br>   - Support des diagrammes<br>   - Intégration avec les docstrings<br><br>3. <strong>Allure Report</strong><br>   - Rapports visuels interactifs<br>   - Historique des exécutions<br>   - Intégration avec les frameworks de test<br><br><h3>Exemple avec Allure</h3><br><br><pre><code>javascript<br>import { test, expect } from '@playwright/test';<br>import { allure } from 'allure-playwright';<br><br>test('User login flow', async ({ page }) => {<br>  await allure.description('Test du processus de connexion utilisateur');<br>  await allure.owner('Team QA');<br>  await allure.tag('smoke', 'authentication');<br>  await allure.severity('critical');<br>  <br>  await allure.step('Navigate to login page', async () => {<br>    await page.goto('/login');<br>  });<br>  <br>  await allure.step('Enter credentials', async () => {<br>    await page.fill('#email', 'test@example.com');<br>    await page.fill('#password', 'password123');<br>  });<br>  <br>  await allure.step('Submit form', async () => {<br>    await page.click('#login-button');<br>  });<br>  <br>  await allure.step('Verify successful login', async () => {<br>    await expect(page).toHaveURL('/dashboard');<br>  });<br>});<br></code></pre><br><br><h2>Points Clés à Retenir</h2><br><br><li>La documentation des tests est un investissement qui améliore la maintenabilité</li><br><li>Utiliser des standards de nommage cohérents et descriptifs</li><br><li>Documenter les données de test et les scénarios</li><br><li>Générer des rapports structurés et exploitables</li><br><li>Utiliser des outils spécialisés pour automatiser la documentation</li><br><li>Maintenir la documentation à jour avec l'évolution des tests</li><br><br><h1>2. Reporting et Analyse des Résultats</h1><br><br><h2>2.1 Types de Rapports de Tests</h2><br><br><h3>Rapports en temps réel</h3><br><br>Les rapports en temps réel permettent un suivi immédiat de l'exécution des tests :<br><br><li><strong>Dashboard live</strong> : Affichage en continu des résultats</li><br><li><strong>Notifications instantanées</strong> : Alertes sur les échecs critiques</li><br><li><strong>Métriques temps réel</strong> : Temps d'exécution, taux de réussite</li><br><br><h3>Rapports post-exécution</h3><br><br>Les rapports détaillés générés après l'exécution complète :<br><br><li><strong>Rapport de synthèse</strong> : Vue d'ensemble des résultats</li><br><li><strong>Rapport détaillé</strong> : Analyse approfondie de chaque test</li><br><li><strong>Rapport de tendances</strong> : Évolution des métriques dans le temps</li><br><br><h3>Rapports par audience</h3><br><br>1. <strong>Rapport développeur</strong><br>   - Détails techniques des échecs<br>   - Stack traces et logs<br>   - Suggestions de correction<br><br>2. <strong>Rapport manager</strong><br>   - Métriques de haut niveau<br>   - Impact sur la livraison<br>   - Tendances qualité<br><br>3. <strong>Rapport métier</strong><br>   - Couverture fonctionnelle<br>   - Risques identifiés<br>   - Conformité aux exigences<br><br><h2>2.2 Métriques Importantes</h2><br><br><h3>Métriques de base</h3><br><br><pre><code>javascript<br>const testMetrics = {<br>  // Métriques d'exécution<br>  totalTests: 150,<br>  passedTests: 142,<br>  failedTests: 6,<br>  skippedTests: 2,<br>  <br>  // Métriques de performance<br>  executionTime: '00:12:34',<br>  averageTestTime: '5.02s',<br>  slowestTest: '45.3s',<br>  <br>  // Métriques de qualité<br>  passRate: 94.7, // %<br>  flakiness: 2.1,  // %<br>  coverage: 87.3   // %<br>};<br></code></pre><br><br><h3>Métriques avancées</h3><br><br>1. <strong>Stabilité des tests</strong><br>   - Taux de flakiness<br>   - Tests intermittents<br>   - Fiabilité par environnement<br><br>2. <strong>Performance des tests</strong><br>   - Temps d'exécution par catégorie<br>   - Évolution des performances<br>   - Goulots d'étranglement<br><br>3. <strong>Couverture et qualité</strong><br>   - Couverture de code<br>   - Couverture fonctionnelle<br>   - Densité de défauts<br><br><h3>Calcul des métriques clés</h3><br><br><pre><code>python<br>def calculate_test_metrics(test_results):<br>    """Calcule les métriques principales des tests"""<br>    <br>    total = len(test_results)<br>    passed = len([t for t in test_results if t.status == 'passed'])<br>    failed = len([t for t in test_results if t.status == 'failed'])<br>    skipped = len([t for t in test_results if t.status == 'skipped'])<br>    <br>    metrics = {<br>        'pass_rate': (passed / total) * 100 if total > 0 else 0,<br>        'fail_rate': (failed / total) * 100 if total > 0 else 0,<br>        'skip_rate': (skipped / total) * 100 if total > 0 else 0,<br>        'total_duration': sum(t.duration for t in test_results),<br>        'average_duration': sum(t.duration for t in test_results) / total if total > 0 else 0<br>    }<br>    <br>    return metrics<br></code></pre><br><br><h2>2.3 Analyse des Tendances</h2><br><br><h3>Suivi historique</h3><br><br>L'analyse des tendances permet d'identifier :<br><br><li><strong>Dégradation de la qualité</strong> : Augmentation du taux d'échec</li><br><li><strong>Amélioration continue</strong> : Réduction des temps d'exécution</li><br><li><strong>Patterns saisonniers</strong> : Variations liées aux releases</li><br><br><h3>Exemple de données de tendance</h3><br><br><pre><code>json<br>{<br>  "trend_data": {<br>    "period": "last_30_days",<br>    "data_points": [<br>      {<br>        "date": "2024-01-01",<br>        "pass_rate": 92.5,<br>        "execution_time": 720,<br>        "total_tests": 145<br>      },<br>      {<br>        "date": "2024-01-02",<br>        "pass_rate": 94.1,<br>        "execution_time": 698,<br>        "total_tests": 147<br>      }<br>    ],<br>    "trends": {<br>      "pass_rate": {<br>        "direction": "improving",<br>        "change_percent": 1.7<br>      },<br>      "execution_time": {<br>        "direction": "improving",<br>        "change_percent": -3.1<br>      }<br>    }<br>  }<br>}<br></code></pre><br><br><h3>Visualisation des tendances</h3><br><br><pre><code>python<br>import matplotlib.pyplot as plt<br>import pandas as pd<br><br>def plot_test_trends(data):<br>    """Génère un graphique des tendances de tests"""<br>    <br>    df = pd.DataFrame(data)<br>    <br>    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))<br>    <br>    # Graphique du taux de réussite<br>    ax1.plot(df['date'], df['pass_rate'], marker='o', color='green')<br>    ax1.set_title('Évolution du Taux de Réussite')<br>    ax1.set_ylabel('Taux de Réussite (%)')<br>    ax1.grid(True)<br>    <br>    # Graphique du temps d'exécution<br>    ax2.plot(df['date'], df['execution_time'], marker='s', color='blue')<br>    ax2.set_title('Évolution du Temps d\'Exécution')<br>    ax2.set_ylabel('Temps (secondes)')<br>    ax2.set_xlabel('Date')<br>    ax2.grid(True)<br>    <br>    plt.tight_layout()<br>    plt.savefig('test_trends.png')<br>    return 'test_trends.png'<br></code></pre><br><br><h2>2.4 Outils de Reporting</h2><br><br><h3>Allure Report</h3><br><br>Allure est un framework de reporting flexible qui génère des rapports HTML interactifs :<br><br><pre><code>javascript<br>// Configuration Allure pour Jest<br>module.exports = {<br>  reporters: [<br>    'default',<br>    ['jest-allure', {<br>      outputDir: 'allure-results',<br>      disableWebdriverStepsReporting: false,<br>      disableWebdriverScreenshotsReporting: false,<br>    }]<br>  ]<br>};<br></code></pre><br><br><strong>Fonctionnalités d'Allure :</strong><br><li>Rapports visuels interactifs</li><br><li>Historique des exécutions</li><br><li>Catégorisation des défauts</li><br><li>Intégration avec CI/CD</li><br><br><h3>ReportPortal</h3><br><br>Plateforme de reporting en temps réel :<br><br><pre><code>yaml<br><h1>reportportal.yml</h1><br>rp:<br>  endpoint: "http://localhost:8080"<br>  project: "my_project"<br>  launch: "Test Execution"<br>  attributes:<br>    - "regression"<br>    - "api"<br></code></pre><br><br><strong>Avantages de ReportPortal :</strong><br><li>Analyse ML des échecs</li><br><li>Clustering automatique des défauts</li><br><li>Intégration avec Jira</li><br><li>Dashboard temps réel</li><br><br><h3>TestRail</h3><br><br>Outil de gestion et reporting de tests :<br><br><pre><code>python<br><h1>Intégration TestRail</h1><br>import testrail<br><br>client = testrail.APIClient('https://company.testrail.io/')<br>client.user = 'user@company.com'<br>client.password = 'password'<br><br><h1>Mise à jour des résultats</h1><br>result = client.send_post(<br>    'add_result_for_case/1/123',<br>    {<br>        'status_id': 1,  # Passed<br>        'comment': 'Test passed successfully',<br>        'elapsed': '5m'<br>    }<br>)<br></code></pre><br><br><h2>2.5 Automatisation du Reporting</h2><br><br><h3>Pipeline de reporting automatisé</h3><br><br><pre><code>yaml<br><h1>.github/workflows/test-reporting.yml</h1><br>name: Test Reporting<br><br>on:<br>  workflow_run:<br>    workflows: ["CI Tests"]<br>    types: [completed]<br><br>jobs:<br>  generate-report:<br>    runs-on: ubuntu-latest<br>    steps:<br>      - uses: actions/checkout@v3<br>      <br>      - name: Download test results<br>        uses: actions/download-artifact@v3<br>        with:<br>          name: test-results<br>          <br>      - name: Generate Allure Report<br>        uses: simple-elf/allure-report-action@master<br>        with:<br>          allure_results: allure-results<br>          allure_history: allure-history<br>          <br>      - name: Deploy to GitHub Pages<br>        uses: peaceiris/actions-gh-pages@v3<br>        with:<br>          github_token: ${{ secrets.GITHUB_TOKEN }}<br>          publish_dir: allure-history<br></code></pre><br><br><h3>Script de génération de rapport personnalisé</h3><br><br><pre><code>python<br>#!/usr/bin/env python3<br>"""<br>Générateur de rapport de tests personnalisé<br>"""<br><br>import json<br>import jinja2<br>from datetime import datetime<br><br>def generate_html_report(test_results, template_path, output_path):<br>    """Génère un rapport HTML à partir des résultats de tests"""<br>    <br>    # Calcul des métriques<br>    metrics = calculate_test_metrics(test_results)<br>    <br>    # Préparation des données pour le template<br>    report_data = {<br>        'timestamp': datetime.now().isoformat(),<br>        'metrics': metrics,<br>        'test_results': test_results,<br>        'failed_tests': [t for t in test_results if t.status == 'failed'],<br>        'slow_tests': sorted(test_results, key=lambda x: x.duration, reverse=True)[:10]<br>    }<br>    <br>    # Génération du rapport<br>    env = jinja2.Environment(loader=jinja2.FileSystemLoader('.'))<br>    template = env.get_template(template_path)<br>    html_content = template.render(<em></em>report_data)<br>    <br>    with open(output_path, 'w', encoding='utf-8') as f:<br>        f.write(html_content)<br>    <br>    print(f"Rapport généré : {output_path}")<br><br>if __name__ == "__main__":<br>    # Chargement des résultats de tests<br>    with open('test-results.json', 'r') as f:<br>        results = json.load(f)<br>    <br>    generate_html_report(results, 'report-template.html', 'test-report.html')<br></code></pre><br><br><h2>2.6 Analyse des Échecs</h2><br><br><h3>Catégorisation automatique</h3><br><br><pre><code>python<br>def categorize_failure(error_message, stack_trace):<br>    """Catégorise automatiquement les échecs de tests"""<br>    <br>    categories = {<br>        'timeout': ['timeout', 'timed out', 'connection timeout'],<br>        'assertion': ['assertion', 'expected', 'actual'],<br>        'network': ['network', 'connection refused', 'dns'],<br>        'environment': ['environment', 'configuration', 'setup'],<br>        'data': ['data', 'database', 'sql']<br>    }<br>    <br>    error_lower = error_message.lower()<br>    <br>    for category, keywords in categories.items():<br>        if any(keyword in error_lower for keyword in keywords):<br>            return category<br>    <br>    return 'unknown'<br></code></pre><br><br><h3>Détection des patterns d'échec</h3><br><br><pre><code>python<br>def detect_failure_patterns(test_history):<br>    """Détecte les patterns récurrents d'échecs"""<br>    <br>    patterns = {}<br>    <br>    for test_run in test_history:<br>        for failed_test in test_run.failed_tests:<br>            test_name = failed_test.name<br>            error_category = categorize_failure(failed_test.error, failed_test.stack_trace)<br>            <br>            if test_name not in patterns:<br>                patterns[test_name] = {}<br>            <br>            if error_category not in patterns[test_name]:<br>                patterns[test_name][error_category] = 0<br>            <br>            patterns[test_name][error_category] += 1<br>    <br>    # Identification des tests problématiques<br>    problematic_tests = []<br>    for test_name, categories in patterns.items():<br>        total_failures = sum(categories.values())<br>        if total_failures > 5:  # Seuil configurable<br>            problematic_tests.append({<br>                'test': test_name,<br>                'total_failures': total_failures,<br>                'main_category': max(categories, key=categories.get)<br>            })<br>    <br>    return problematic_tests<br></code></pre><br><br><h2>Points Clés à Retenir</h2><br><br><li>Adapter les rapports à l'audience cible (développeur, manager, métier)</li><br><li>Suivre les métriques clés : taux de réussite, temps d'exécution, stabilité</li><br><li>Analyser les tendances pour identifier les problèmes émergents</li><br><li>Automatiser la génération et la distribution des rapports</li><br><li>Catégoriser les échecs pour faciliter l'analyse</li><br><li>Utiliser des outils spécialisés comme Allure ou ReportPortal</li><br><li>Intégrer le reporting dans le pipeline CI/CD</li><br><br><h1>3. Monitoring des Tests avec Dashboards</h1><br><br><h2>3.1 Principes du Monitoring</h2><br><br><h3>Pourquoi monitorer les tests ?</h3><br><br>Le monitoring des tests automatisés permet de :<br><br><li><strong>Détecter rapidement les problèmes</strong> : Identification immédiate des régressions</li><br><li><strong>Optimiser les performances</strong> : Suivi des temps d'exécution et goulots d'étranglement</li><br><li><strong>Assurer la stabilité</strong> : Surveillance de la fiabilité des tests</li><br><li><strong>Faciliter la prise de décision</strong> : Données objectives pour les équipes</li><br><br><h3>Approche proactive vs réactive</h3><br><br><strong>Monitoring proactif :</strong><br><li>Surveillance continue des métriques</li><br><li>Alertes préventives sur les seuils</li><br><li>Analyse des tendances</li><br><li>Prédiction des problèmes</li><br><br><strong>Monitoring réactif :</strong><br><li>Réaction aux échecs de tests</li><br><li>Analyse post-mortem</li><br><li>Correction après incident</li><br><li>Impact sur la livraison</li><br><br><h2>3.2 Métriques de Monitoring</h2><br><br><h3>Métriques de performance</h3><br><br><pre><code>javascript<br>const performanceMetrics = {<br>  // Temps d'exécution<br>  executionTime: {<br>    total: '00:15:42',<br>    average: '6.2s',<br>    median: '4.1s',<br>    p95: '18.3s',<br>    p99: '45.7s'<br>  },<br>  <br>  // Utilisation des ressources<br>  resources: {<br>    cpuUsage: 65.4,      // %<br>    memoryUsage: 2.1,    // GB<br>    diskIO: 45.2,        // MB/s<br>    networkIO: 12.8      // MB/s<br>  },<br>  <br>  // Parallélisation<br>  concurrency: {<br>    maxWorkers: 8,<br>    avgWorkers: 6.2,<br>    queueTime: '2.3s'<br>  }<br>};<br></code></pre><br><br><h3>Métriques de qualité</h3><br><br><pre><code>python<br>quality_metrics = {<br>    # Stabilité des tests<br>    'flakiness_rate': 2.1,        # %<br>    'consistency_score': 94.7,    # %<br>    'reliability_index': 0.947,   # 0-1<br>    <br>    # Couverture<br>    'code_coverage': 87.3,        # %<br>    'functional_coverage': 92.1,  # %<br>    'requirement_coverage': 89.5, # %<br>    <br>    # Défauts<br>    'defect_detection_rate': 78.2,  # %<br>    'false_positive_rate': 3.4,     # %<br>    'escape_rate': 1.2               # %<br>}<br></code></pre><br><br><h3>Métriques métier</h3><br><br><pre><code>yaml<br>business_metrics:<br>  deployment_frequency: "2.3/day"<br>  lead_time: "4.2 hours"<br>  mttr: "1.8 hours"          # Mean Time To Recovery<br>  change_failure_rate: "2.1%" # %<br>  <br>  quality_gates:<br>    - name: "Unit Tests"<br>      threshold: 95<br>      current: 97.2<br>      status: "passed"<br>    - name: "Integration Tests"<br>      threshold: 90<br>      current: 88.5<br>      status: "failed"<br></code></pre><br><br><h2>3.3 Architecture de Monitoring</h2><br><br><h3>Stack de monitoring moderne</h3><br><br><pre><code>mermaid<br>graph TB<br>    A[Tests Automatisés] --> B[Collecteurs de Métriques]<br>    B --> C[Base de Données Métriques]<br>    C --> D[Dashboards]<br>    C --> E[Alerting]<br>    <br>    B --> F[Prometheus]<br>    B --> G[InfluxDB]<br>    B --> H[Elasticsearch]<br>    <br>    D --> I[Grafana]<br>    D --> J[Kibana]<br>    D --> K[Custom Dashboards]<br>    <br>    E --> L[AlertManager]<br>    E --> M[PagerDuty]<br>    E --> N[Slack/Teams]<br></code></pre><br><br><h3>Collecte des métriques</h3><br><br><pre><code>python<br>import time<br>import psutil<br>from prometheus_client import Counter, Histogram, Gauge, start_http_server<br><br><h1>Métriques Prometheus</h1><br>test_counter = Counter('tests_total', 'Total number of tests', ['status', 'suite'])<br>test_duration = Histogram('test_duration_seconds', 'Test execution time', ['test_name'])<br>active_tests = Gauge('active_tests', 'Number of currently running tests')<br><br>class TestMonitor:<br>    def __init__(self):<br>        self.start_time = None<br>        <br>    def start_test(self, test_name):<br>        """Démarre le monitoring d'un test"""<br>        self.start_time = time.time()<br>        active_tests.inc()<br>        <br>    def end_test(self, test_name, status, suite):<br>        """Termine le monitoring d'un test"""<br>        if self.start_time:<br>            duration = time.time() - self.start_time<br>            test_duration.labels(test_name=test_name).observe(duration)<br>            <br>        test_counter.labels(status=status, suite=suite).inc()<br>        active_tests.dec()<br>        <br>    def collect_system_metrics(self):<br>        """Collecte les métriques système"""<br>        return {<br>            'cpu_percent': psutil.cpu_percent(),<br>            'memory_percent': psutil.virtual_memory().percent,<br>            'disk_usage': psutil.disk_usage('/').percent<br>        }<br><br><h1>Démarrage du serveur de métriques</h1><br>if __name__ == "__main__":<br>    start_http_server(8000)<br>    monitor = TestMonitor()<br></code></pre><br><br><h2>3.4 Dashboards avec Grafana</h2><br><br><h3>Configuration de base</h3><br><br><pre><code>yaml<br><h1>docker-compose.yml pour stack monitoring</h1><br>version: '3.8'<br>services:<br>  prometheus:<br>    image: prom/prometheus:latest<br>    ports:<br>      - "9090:9090"<br>    volumes:<br>      - ./prometheus.yml:/etc/prometheus/prometheus.yml<br>      <br>  grafana:<br>    image: grafana/grafana:latest<br>    ports:<br>      - "3000:3000"<br>    environment:<br>      - GF_SECURITY_ADMIN_PASSWORD=admin<br>    volumes:<br>      - grafana-storage:/var/lib/grafana<br>      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards<br>      - ./grafana/datasources:/etc/grafana/provisioning/datasources<br><br>volumes:<br>  grafana-storage:<br></code></pre><br><br><h3>Configuration Prometheus</h3><br><br><pre><code>yaml<br><h1>prometheus.yml</h1><br>global:<br>  scrape_interval: 15s<br><br>scrape_configs:<br>  - job_name: 'test-metrics'<br>    static_configs:<br>      - targets: ['localhost:8000']<br>    scrape_interval: 5s<br>    <br>  - job_name: 'jenkins'<br>    static_configs:<br>      - targets: ['jenkins:8080']<br>    metrics_path: '/prometheus'<br>    <br>  - job_name: 'node-exporter'<br>    static_configs:<br>      - targets: ['node-exporter:9100']<br></code></pre><br><br><h3>Dashboard JSON pour Grafana</h3><br><br><pre><code>json<br>{<br>  "dashboard": {<br>    "title": "Test Execution Dashboard",<br>    "panels": [<br>      {<br>        "title": "Test Success Rate",<br>        "type": "stat",<br>        "targets": [<br>          {<br>            "expr": "rate(tests_total{status=\"passed\"}[5m]) / rate(tests_total[5m]) * 100",<br>            "legendFormat": "Success Rate %"<br>          }<br>        ],<br>        "fieldConfig": {<br>          "defaults": {<br>            "unit": "percent",<br>            "thresholds": {<br>              "steps": [<br>                {"color": "red", "value": 0},<br>                {"color": "yellow", "value": 80},<br>                {"color": "green", "value": 95}<br>              ]<br>            }<br>          }<br>        }<br>      },<br>      {<br>        "title": "Test Execution Time",<br>        "type": "graph",<br>        "targets": [<br>          {<br>            "expr": "histogram_quantile(0.95, test_duration_seconds_bucket)",<br>            "legendFormat": "95th percentile"<br>          },<br>          {<br>            "expr": "histogram_quantile(0.50, test_duration_seconds_bucket)",<br>            "legendFormat": "Median"<br>          }<br>        ]<br>      }<br>    ]<br>  }<br>}<br></code></pre><br><br><h2>3.5 Alerting et Notifications</h2><br><br><h3>Configuration des alertes</h3><br><br><pre><code>yaml<br><h1>alertmanager.yml</h1><br>global:<br>  smtp_smarthost: 'localhost:587'<br>  smtp_from: 'alerts@company.com'<br><br>route:<br>  group_by: ['alertname']<br>  group_wait: 10s<br>  group_interval: 10s<br>  repeat_interval: 1h<br>  receiver: 'web.hook'<br><br>receivers:<br><li>name: 'web.hook'</li><br>  slack_configs:<br>  - api_url: 'https://hooks.slack.com/services/...'<br>    channel: '#test-alerts'<br>    title: 'Test Alert'<br>    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'<br></code></pre><br><br><h3>Règles d'alerte Prometheus</h3><br><br><pre><code>yaml<br><h1>alert-rules.yml</h1><br>groups:<br><li>name: test-alerts</li><br>  rules:<br>  - alert: HighTestFailureRate<br>    expr: rate(tests_total{status="failed"}[5m]) / rate(tests_total[5m]) > 0.1<br>    for: 2m<br>    labels:<br>      severity: warning<br>    annotations:<br>      summary: "High test failure rate detected"<br>      description: "Test failure rate is {{ $value | humanizePercentage }}"<br>      <br>  - alert: SlowTestExecution<br>    expr: histogram_quantile(0.95, test_duration_seconds_bucket) > 60<br>    for: 5m<br>    labels:<br>      severity: critical<br>    annotations:<br>      summary: "Tests are running slowly"<br>      description: "95th percentile execution time is {{ $value }}s"<br>      <br>  - alert: TestSuiteDown<br>    expr: up{job="test-metrics"} == 0<br>    for: 1m<br>    labels:<br>      severity: critical<br>    annotations:<br>      summary: "Test suite monitoring is down"<br></code></pre><br><br><h2>3.6 Dashboards Personnalisés</h2><br><br><h3>Dashboard React personnalisé</h3><br><br><pre><code>jsx<br>import React, { useState, useEffect } from 'react';<br>import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, Legend } from 'recharts';<br><br>const TestDashboard = () => {<br>  const [metrics, setMetrics] = useState([]);<br>  const [realTimeData, setRealTimeData] = useState({});<br><br>  useEffect(() => {<br>    // Connexion WebSocket pour données temps réel<br>    const ws = new WebSocket('ws://localhost:8080/metrics');<br>    <br>    ws.onmessage = (event) => {<br>      const data = JSON.parse(event.data);<br>      setRealTimeData(data);<br>    };<br><br>    // Chargement des données historiques<br>    fetch('/api/metrics/history')<br>      .then(response => response.json())<br>      .then(data => setMetrics(data));<br><br>    return () => ws.close();<br>  }, []);<br><br>  return (<br>    <div className="dashboard"><br>      <div className="metrics-grid"><br>        <div className="metric-card"><br>          <h3>Taux de Réussite</h3><br>          <div className="metric-value"><br>            {realTimeData.successRate?.toFixed(1)}%<br>          </div><br>        </div><br>        <br>        <div className="metric-card"><br>          <h3>Tests Actifs</h3><br>          <div className="metric-value"><br>            {realTimeData.activeTests || 0}<br>          </div><br>        </div><br>        <br>        <div className="metric-card"><br>          <h3>Temps Moyen</h3><br>          <div className="metric-value"><br>            {realTimeData.avgDuration?.toFixed(1)}s<br>          </div><br>        </div><br>      </div><br>      <br>      <div className="charts"><br>        <LineChart width={800} height={300} data={metrics}><br>          <CartesianGrid strokeDasharray="3 3" /><br>          <XAxis dataKey="timestamp" /><br>          <YAxis /><br>          <Tooltip /><br>          <Legend /><br>          <Line type="monotone" dataKey="successRate" stroke="#8884d8" /><br>          <Line type="monotone" dataKey="executionTime" stroke="#82ca9d" /><br>        </LineChart><br>      </div><br>    </div><br>  );<br>};<br><br>export default TestDashboard;<br></code></pre><br><br><h3>API pour métriques personnalisées</h3><br><br><pre><code>python<br>from flask import Flask, jsonify, request<br>from flask_socketio import SocketIO, emit<br>import json<br>from datetime import datetime, timedelta<br><br>app = Flask(__name__)<br>socketio = SocketIO(app, cors_allowed_origins="*")<br><br>class MetricsCollector:<br>    def __init__(self):<br>        self.metrics_history = []<br>        self.current_metrics = {}<br>    <br>    def add_metric(self, metric_data):<br>        """Ajoute une nouvelle métrique"""<br>        metric_data['timestamp'] = datetime.now().isoformat()<br>        self.metrics_history.append(metric_data)<br>        self.current_metrics = metric_data<br>        <br>        # Diffusion temps réel<br>        socketio.emit('metrics_update', metric_data)<br>    <br>    def get_history(self, hours=24):<br>        """Récupère l'historique des métriques"""<br>        cutoff = datetime.now() - timedelta(hours=hours)<br>        return [<br>            m for m in self.metrics_history <br>            if datetime.fromisoformat(m['timestamp']) > cutoff<br>        ]<br><br>collector = MetricsCollector()<br><br>@app.route('/api/metrics/current')<br>def get_current_metrics():<br>    return jsonify(collector.current_metrics)<br><br>@app.route('/api/metrics/history')<br>def get_metrics_history():<br>    hours = request.args.get('hours', 24, type=int)<br>    return jsonify(collector.get_history(hours))<br><br>@app.route('/api/metrics', methods=['POST'])<br>def post_metrics():<br>    data = request.json<br>    collector.add_metric(data)<br>    return jsonify({'status': 'success'})<br><br>if __name__ == '__main__':<br>    socketio.run(app, debug=True, port=8080)<br></code></pre><br><br><h2>3.7 Monitoring des Environnements</h2><br><br><h3>Surveillance multi-environnements</h3><br><br><pre><code>python<br>import requests<br>from dataclasses import dataclass<br>from typing import Dict, List<br><br>@dataclass<br>class Environment:<br>    name: str<br>    url: str<br>    expected_response_time: float<br>    health_endpoint: str<br><br>class EnvironmentMonitor:<br>    def __init__(self, environments: List[Environment]):<br>        self.environments = environments<br>        <br>    def check_environment_health(self, env: Environment) -> Dict:<br>        """Vérifie la santé d'un environnement"""<br>        try:<br>            start_time = time.time()<br>            response = requests.get(f"{env.url}{env.health_endpoint}", timeout=10)<br>            response_time = time.time() - start_time<br>            <br>            return {<br>                'environment': env.name,<br>                'status': 'healthy' if response.status_code == 200 else 'unhealthy',<br>                'response_time': response_time,<br>                'status_code': response.status_code,<br>                'within_sla': response_time <= env.expected_response_time<br>            }<br>        except Exception as e:<br>            return {<br>                'environment': env.name,<br>                'status': 'error',<br>                'error': str(e),<br>                'response_time': None,<br>                'within_sla': False<br>            }<br>    <br>    def monitor_all_environments(self) -> List[Dict]:<br>        """Surveille tous les environnements"""<br>        results = []<br>        for env in self.environments:<br>            result = self.check_environment_health(env)<br>            results.append(result)<br>        return results<br><br><h1>Configuration des environnements</h1><br>environments = [<br>    Environment("dev", "https://dev.api.com", 2.0, "/health"),<br>    Environment("staging", "https://staging.api.com", 1.5, "/health"),<br>    Environment("prod", "https://api.com", 1.0, "/health")<br>]<br><br>monitor = EnvironmentMonitor(environments)<br></code></pre><br><br><h2>Points Clés à Retenir</h2><br><br><li>Le monitoring proactif permet de détecter les problèmes avant qu'ils impactent la production</li><br><li>Utiliser des métriques variées : performance, qualité, métier</li><br><li>Grafana et Prometheus forment un stack puissant pour le monitoring</li><br><li>Configurer des alertes pertinentes pour éviter la fatigue d'alerte</li><br><li>Les dashboards doivent être adaptés à l'audience (développeurs, managers, ops)</li><br><li>Surveiller les environnements de test pour assurer leur disponibilité</li><br><li>Automatiser la collecte et l'analyse des métriques</li><br><li>Intégrer le monitoring dans le pipeline CI/CD</li><br><br><h1>4. Outils : Allure, Grafana, Prometheus</h1><br><br><h2>4.1 Allure Report - Reporting Avancé</h2><br><br><h3>Introduction à Allure</h3><br><br>Allure est un framework de reporting flexible qui génère des rapports de tests visuels et interactifs. Il supporte de nombreux frameworks de test et langages de programmation.<br><br><strong>Avantages d'Allure :</strong><br><li>Rapports HTML interactifs et esthétiques</li><br><li>Historique des exécutions</li><br><li>Catégorisation automatique des défauts</li><br><li>Intégration avec les frameworks de test populaires</li><br><li>Support des attachments (screenshots, logs, vidéos)</li><br><br><h3>Installation et Configuration</h3><br><br>#### Installation via npm<br><br><pre><code>bash<br><h1>Installation globale</h1><br>npm install -g allure-commandline<br><br><h1>Vérification de l'installation</h1><br>allure --version<br></code></pre><br><br>#### Configuration pour Jest<br><br><pre><code>javascript<br>// jest.config.js<br>module.exports = {<br>  reporters: [<br>    'default',<br>    ['jest-allure', {<br>      outputDir: 'allure-results',<br>      disableWebdriverStepsReporting: false,<br>      disableWebdriverScreenshotsReporting: false,<br>    }]<br>  ],<br>  setupFilesAfterEnv: ['<rootDir>/test-setup.js']<br>};<br></code></pre><br><br><pre><code>javascript<br>// test-setup.js<br>const { registerAllureReporter } = require('jest-allure/dist/setup');<br>registerAllureReporter();<br></code></pre><br><br>#### Configuration pour Playwright<br><br><pre><code>javascript<br>// playwright.config.js<br>module.exports = {<br>  reporter: [<br>    ['line'],<br>    ['allure-playwright', { <br>      outputFolder: 'allure-results',<br>      suiteTitle: false <br>    }]<br>  ],<br>  use: {<br>    screenshot: 'only-on-failure',<br>    video: 'retain-on-failure',<br>  }<br>};<br></code></pre><br><br><h3>Utilisation Avancée d'Allure</h3><br><br>#### Annotations et métadonnées<br><br><pre><code>javascript<br>import { test, expect } from '@playwright/test';<br>import { allure } from 'allure-playwright';<br><br>test('User authentication flow', async ({ page }) => {<br>  // Métadonnées du test<br>  await allure.description('Test complet du processus d\'authentification utilisateur');<br>  await allure.owner('Team QA');<br>  await allure.tag('smoke', 'authentication', 'critical');<br>  await allure.severity('critical');<br>  await allure.story('User Login');<br>  await allure.feature('Authentication');<br>  <br>  // Lien vers les exigences<br>  await allure.link('https://jira.company.com/REQ-123', 'Requirement');<br>  await allure.issue('https://jira.company.com/BUG-456', 'Related Bug');<br>  <br>  await allure.step('Navigate to login page', async () => {<br>    await page.goto('/login');<br>    await allure.attachment('Login Page Screenshot', await page.screenshot(), 'image/png');<br>  });<br>  <br>  await allure.step('Enter valid credentials', async () => {<br>    await page.fill('#email', 'test@example.com');<br>    await page.fill('#password', 'password123');<br>  });<br>  <br>  await allure.step('Submit login form', async () => {<br>    await page.click('#login-button');<br>  });<br>  <br>  await allure.step('Verify successful login', async () => {<br>    await expect(page).toHaveURL('/dashboard');<br>    await allure.attachment('Dashboard Screenshot', await page.screenshot(), 'image/png');<br>  });<br>});<br></code></pre><br><br>#### Catégorisation des défauts<br><br><pre><code>javascript<br>// categories.json<br>[<br>  {<br>    "name": "Ignored tests",<br>    "matchedStatuses": ["skipped"]<br>  },<br>  {<br>    "name": "Infrastructure problems",<br>    "matchedStatuses": ["broken", "failed"],<br>    "messageRegex": ".<em>timeout.</em>|.<em>connection.</em>|.<em>network.</em>"<br>  },<br>  {<br>    "name": "Outdated tests",<br>    "matchedStatuses": ["broken"],<br>    "traceRegex": ".<em>NoSuchElementException.</em>"<br>  },<br>  {<br>    "name": "Product defects",<br>    "matchedStatuses": ["failed"]<br>  }<br>]<br></code></pre><br><br><h3>Génération et Déploiement des Rapports</h3><br><br>#### Script de génération<br><br><pre><code>bash<br>#!/bin/bash<br><h1>generate-allure-report.sh</h1><br><br><h1>Nettoyage des anciens résultats</h1><br>rm -rf allure-results allure-report<br><br><h1>Exécution des tests</h1><br>npm test<br><br><h1>Génération du rapport</h1><br>allure generate allure-results --clean -o allure-report<br><br><h1>Ouverture du rapport</h1><br>allure open allure-report<br></code></pre><br><br>#### Intégration CI/CD avec GitHub Actions<br><br><pre><code>yaml<br><h1>.github/workflows/test-report.yml</h1><br>name: Test and Generate Report<br><br>on: [push, pull_request]<br><br>jobs:<br>  test:<br>    runs-on: ubuntu-latest<br>    <br>    steps:<br>    - uses: actions/checkout@v3<br>    <br>    - name: Setup Node.js<br>      uses: actions/setup-node@v3<br>      with:<br>        node-version: '18'<br>        <br>    - name: Install dependencies<br>      run: npm ci<br>      <br>    - name: Run tests<br>      run: npm test<br>      continue-on-error: true<br>      <br>    - name: Get Allure history<br>      uses: actions/checkout@v3<br>      if: always()<br>      continue-on-error: true<br>      with:<br>        ref: gh-pages<br>        path: gh-pages<br>        <br>    - name: Allure Report action<br>      uses: simple-elf/allure-report-action@master<br>      if: always()<br>      with:<br>        allure_results: allure-results<br>        allure_history: allure-history<br>        keep_reports: 20<br>        <br>    - name: Deploy to GitHub Pages<br>      uses: peaceiris/actions-gh-pages@v3<br>      if: always()<br>      with:<br>        github_token: ${{ secrets.GITHUB_TOKEN }}<br>        publish_dir: allure-history<br></code></pre><br><br><h2>4.2 Prometheus - Collecte de Métriques</h2><br><br><h3>Introduction à Prometheus</h3><br><br>Prometheus est un système de monitoring et d'alerting open-source conçu pour la fiabilité et la scalabilité. Il collecte et stocke les métriques sous forme de séries temporelles.<br><br><strong>Caractéristiques clés :</strong><br><li>Modèle de données multidimensionnel</li><br><li>Langage de requête puissant (PromQL)</li><br><li>Collecte par scraping HTTP</li><br><li>Découverte de services automatique</li><br><li>Alerting intégré</li><br><br><h3>Installation et Configuration</h3><br><br>#### Installation avec Docker<br><br><pre><code>yaml<br><h1>docker-compose.yml</h1><br>version: '3.8'<br>services:<br>  prometheus:<br>    image: prom/prometheus:latest<br>    container_name: prometheus<br>    ports:<br>      - "9090:9090"<br>    volumes:<br>      - ./prometheus.yml:/etc/prometheus/prometheus.yml<br>      - ./alert-rules.yml:/etc/prometheus/alert-rules.yml<br>      - prometheus-data:/prometheus<br>    command:<br>      - '--config.file=/etc/prometheus/prometheus.yml'<br>      - '--storage.tsdb.path=/prometheus'<br>      - '--web.console.libraries=/etc/prometheus/console_libraries'<br>      - '--web.console.templates=/etc/prometheus/consoles'<br>      - '--storage.tsdb.retention.time=200h'<br>      - '--web.enable-lifecycle'<br>      - '--web.enable-admin-api'<br><br>volumes:<br>  prometheus-data:<br></code></pre><br><br>#### Configuration de base<br><br><pre><code>yaml<br><h1>prometheus.yml</h1><br>global:<br>  scrape_interval: 15s<br>  evaluation_interval: 15s<br><br>rule_files:<br>  - "alert-rules.yml"<br><br>alerting:<br>  alertmanagers:<br>    - static_configs:<br>        - targets:<br>          - alertmanager:9093<br><br>scrape_configs:<br>  - job_name: 'prometheus'<br>    static_configs:<br>      - targets: ['localhost:9090']<br><br>  - job_name: 'test-metrics'<br>    static_configs:<br>      - targets: ['localhost:8000']<br>    scrape_interval: 5s<br>    metrics_path: '/metrics'<br>    <br>  - job_name: 'node-exporter'<br>    static_configs:<br>      - targets: ['node-exporter:9100']<br></code></pre><br><br><h3>Exposition de Métriques de Tests</h3><br><br>#### Client Python<br><br><pre><code>python<br>from prometheus_client import Counter, Histogram, Gauge, start_http_server<br>import time<br>import random<br><br><h1>Définition des métriques</h1><br>test_counter = Counter(<br>    'tests_total', <br>    'Total number of tests executed',<br>    ['status', 'suite', 'environment']<br>)<br><br>test_duration = Histogram(<br>    'test_duration_seconds',<br>    'Time spent executing tests',<br>    ['test_name', 'suite'],<br>    buckets=[0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, float('inf')]<br>)<br><br>active_tests = Gauge(<br>    'active_tests_count',<br>    'Number of currently running tests'<br>)<br><br>test_queue_size = Gauge(<br>    'test_queue_size',<br>    'Number of tests waiting to be executed'<br>)<br><br>class TestMetricsCollector:<br>    def __init__(self):<br>        self.test_start_times = {}<br>        <br>    def start_test(self, test_name, suite):<br>        """Marque le début d'un test"""<br>        self.test_start_times[test_name] = time.time()<br>        active_tests.inc()<br>        <br>    def end_test(self, test_name, suite, status, environment='test'):<br>        """Marque la fin d'un test"""<br>        if test_name in self.test_start_times:<br>            duration = time.time() - self.test_start_times[test_name]<br>            test_duration.labels(test_name=test_name, suite=suite).observe(duration)<br>            del self.test_start_times[test_name]<br>            <br>        test_counter.labels(status=status, suite=suite, environment=environment).inc()<br>        active_tests.dec()<br>        <br>    def update_queue_size(self, size):<br>        """Met à jour la taille de la queue"""<br>        test_queue_size.set(size)<br><br><h1>Simulation de tests</h1><br>def simulate_test_execution():<br>    collector = TestMetricsCollector()<br>    <br>    test_suites = ['unit', 'integration', 'e2e']<br>    test_names = [f'test_{i}' for i in range(1, 21)]<br>    <br>    while True:<br>        suite = random.choice(test_suites)<br>        test_name = random.choice(test_names)<br>        <br>        collector.start_test(test_name, suite)<br>        <br>        # Simulation du temps d'exécution<br>        execution_time = random.uniform(0.5, 10.0)<br>        time.sleep(execution_time)<br>        <br>        # Simulation du résultat<br>        status = random.choices(['passed', 'failed', 'skipped'], weights=[85, 10, 5])[0]<br>        collector.end_test(test_name, suite, status)<br>        <br>        # Mise à jour de la queue<br>        collector.update_queue_size(random.randint(0, 50))<br>        <br>        time.sleep(1)<br><br>if __name__ == '__main__':<br>    # Démarrage du serveur de métriques<br>    start_http_server(8000)<br>    print("Serveur de métriques démarré sur le port 8000")<br>    <br>    # Simulation des tests<br>    simulate_test_execution()<br></code></pre><br><br>#### Client Node.js<br><br><pre><code>javascript<br>const client = require('prom-client');<br>const express = require('express');<br><br>// Création du registre<br>const register = new client.Registry();<br><br>// Métriques personnalisées<br>const testCounter = new client.Counter({<br>  name: 'tests_total',<br>  help: 'Total number of tests executed',<br>  labelNames: ['status', 'suite', 'environment'],<br>  registers: [register]<br>});<br><br>const testDuration = new client.Histogram({<br>  name: 'test_duration_seconds',<br>  help: 'Time spent executing tests',<br>  labelNames: ['test_name', 'suite'],<br>  buckets: [0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0],<br>  registers: [register]<br>});<br><br>const activeTests = new client.Gauge({<br>  name: 'active_tests_count',<br>  help: 'Number of currently running tests',<br>  registers: [register]<br>});<br><br>// Métriques par défaut<br>client.collectDefaultMetrics({ register });<br><br>class TestMetricsCollector {<br>  constructor() {<br>    this.testStartTimes = new Map();<br>  }<br>  <br>  startTest(testName, suite) {<br>    this.testStartTimes.set(testName, Date.now());<br>    activeTests.inc();<br>  }<br>  <br>  endTest(testName, suite, status, environment = 'test') {<br>    if (this.testStartTimes.has(testName)) {<br>      const duration = (Date.now() - this.testStartTimes.get(testName)) / 1000;<br>      testDuration.labels(testName, suite).observe(duration);<br>      this.testStartTimes.delete(testName);<br>    }<br>    <br>    testCounter.labels(status, suite, environment).inc();<br>    activeTests.dec();<br>  }<br>}<br><br>// Serveur Express pour exposer les métriques<br>const app = express();<br>const collector = new TestMetricsCollector();<br><br>app.get('/metrics', async (req, res) => {<br>  res.set('Content-Type', register.contentType);<br>  res.end(await register.metrics());<br>});<br><br>// Simulation de tests<br>function simulateTests() {<br>  const suites = ['unit', 'integration', 'e2e'];<br>  const testNames = Array.from({length: 20}, (_, i) => <code>test_${i + 1}</code>);<br>  <br>  setInterval(() => {<br>    const suite = suites[Math.floor(Math.random() * suites.length)];<br>    const testName = testNames[Math.floor(Math.random() * testNames.length)];<br>    <br>    collector.startTest(testName, suite);<br>    <br>    // Simulation du temps d'exécution<br>    const executionTime = Math.random() * 5000 + 500;<br>    setTimeout(() => {<br>      const statuses = ['passed', 'failed', 'skipped'];<br>      const weights = [0.85, 0.10, 0.05];<br>      const random = Math.random();<br>      let status = 'passed';<br>      <br>      if (random > 0.85) status = random > 0.95 ? 'skipped' : 'failed';<br>      <br>      collector.endTest(testName, suite, status);<br>    }, executionTime);<br>  }, 1000);<br>}<br><br>const PORT = process.env.PORT || 8000;<br>app.listen(PORT, () => {<br>  console.log(<code>Serveur de métriques démarré sur le port ${PORT}</code>);<br>  simulateTests();<br>});<br></code></pre><br><br><h3>Requêtes PromQL Utiles</h3><br><br><pre><code>promql<br><h1>Taux de réussite des tests sur 5 minutes</h1><br>rate(tests_total{status="passed"}[5m]) / rate(tests_total[5m]) * 100<br><br><h1>Temps d'exécution médian par suite</h1><br>histogram_quantile(0.5, rate(test_duration_seconds_bucket[5m]))<br><br><h1>Tests les plus lents (95e percentile)</h1><br>histogram_quantile(0.95, rate(test_duration_seconds_bucket[5m]))<br><br><h1>Nombre de tests actifs</h1><br>active_tests_count<br><br><h1>Évolution du taux d'échec</h1><br>increase(tests_total{status="failed"}[1h])<br><br><h1>Tests par environnement</h1><br>sum by (environment) (rate(tests_total[5m]))<br><br><h1>Détection d'anomalies (tests inhabituellement lents)</h1><br>test_duration_seconds > on() group_left() (<br>  avg_over_time(test_duration_seconds[1h]) + 2 * stddev_over_time(test_duration_seconds[1h])<br>)<br></code></pre><br><br><h2>4.3 Grafana - Visualisation et Dashboards</h2><br><br><h3>Introduction à Grafana</h3><br><br>Grafana est une plateforme de visualisation et d'observabilité qui permet de créer des dashboards interactifs à partir de multiples sources de données.<br><br><strong>Fonctionnalités principales :</strong><br><li>Dashboards interactifs et personnalisables</li><br><li>Support de nombreuses sources de données</li><br><li>Système d'alerting avancé</li><br><li>Gestion des utilisateurs et permissions</li><br><li>API REST complète</li><br><br><h3>Installation et Configuration</h3><br><br>#### Installation avec Docker<br><br><pre><code>yaml<br><h1>docker-compose.yml (complet avec Prometheus)</h1><br>version: '3.8'<br>services:<br>  prometheus:<br>    image: prom/prometheus:latest<br>    ports:<br>      - "9090:9090"<br>    volumes:<br>      - ./prometheus.yml:/etc/prometheus/prometheus.yml<br>      - prometheus-data:/prometheus<br>    command:<br>      - '--config.file=/etc/prometheus/prometheus.yml'<br>      - '--storage.tsdb.path=/prometheus'<br>      - '--web.enable-lifecycle'<br><br>  grafana:<br>    image: grafana/grafana:latest<br>    ports:<br>      - "3000:3000"<br>    environment:<br>      - GF_SECURITY_ADMIN_USER=admin<br>      - GF_SECURITY_ADMIN_PASSWORD=admin123<br>      - GF_USERS_ALLOW_SIGN_UP=false<br>    volumes:<br>      - grafana-data:/var/lib/grafana<br>      - ./grafana/provisioning:/etc/grafana/provisioning<br>      - ./grafana/dashboards:/var/lib/grafana/dashboards<br>    depends_on:<br>      - prometheus<br><br>  node-exporter:<br>    image: prom/node-exporter:latest<br>    ports:<br>      - "9100:9100"<br>    volumes:<br>      - /proc:/host/proc:ro<br>      - /sys:/host/sys:ro<br>      - /:/rootfs:ro<br>    command:<br>      - '--path.procfs=/host/proc'<br>      - '--path.rootfs=/rootfs'<br>      - '--path.sysfs=/host/sys'<br>      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'<br><br>volumes:<br>  prometheus-data:<br>  grafana-data:<br></code></pre><br><br>#### Configuration des sources de données<br><br><pre><code>yaml<br><h1>grafana/provisioning/datasources/prometheus.yml</h1><br>apiVersion: 1<br><br>datasources:<br>  - name: Prometheus<br>    type: prometheus<br>    access: proxy<br>    url: http://prometheus:9090<br>    isDefault: true<br>    editable: true<br></code></pre><br><br><h3>Création de Dashboards</h3><br><br>#### Dashboard JSON pour tests<br><br><pre><code>json<br>{<br>  "dashboard": {<br>    "id": null,<br>    "title": "Test Execution Dashboard",<br>    "tags": ["testing", "ci-cd"],<br>    "timezone": "browser",<br>    "panels": [<br>      {<br>        "id": 1,<br>        "title": "Test Success Rate",<br>        "type": "stat",<br>        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0},<br>        "targets": [<br>          {<br>            "expr": "rate(tests_total{status=\"passed\"}[5m]) / rate(tests_total[5m]) * 100",<br>            "legendFormat": "Success Rate"<br>          }<br>        ],<br>        "fieldConfig": {<br>          "defaults": {<br>            "unit": "percent",<br>            "min": 0,<br>            "max": 100,<br>            "thresholds": {<br>              "steps": [<br>                {"color": "red", "value": 0},<br>                {"color": "yellow", "value": 80},<br>                {"color": "green", "value": 95}<br>              ]<br>            }<br>          }<br>        }<br>      },<br>      {<br>        "id": 2,<br>        "title": "Active Tests",<br>        "type": "stat",<br>        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0},<br>        "targets": [<br>          {<br>            "expr": "active_tests_count",<br>            "legendFormat": "Active Tests"<br>          }<br>        ],<br>        "fieldConfig": {<br>          "defaults": {<br>            "unit": "short",<br>            "thresholds": {<br>              "steps": [<br>                {"color": "green", "value": 0},<br>                {"color": "yellow", "value": 10},<br>                {"color": "red", "value": 20}<br>              ]<br>            }<br>          }<br>        }<br>      },<br>      {<br>        "id": 3,<br>        "title": "Test Execution Time",<br>        "type": "timeseries",<br>        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},<br>        "targets": [<br>          {<br>            "expr": "histogram_quantile(0.95, rate(test_duration_seconds_bucket[5m]))",<br>            "legendFormat": "95th percentile"<br>          },<br>          {<br>            "expr": "histogram_quantile(0.50, rate(test_duration_seconds_bucket[5m]))",<br>            "legendFormat": "Median"<br>          }<br>        ],<br>        "fieldConfig": {<br>          "defaults": {<br>            "unit": "s",<br>            "custom": {<br>              "drawStyle": "line",<br>              "lineInterpolation": "linear",<br>              "fillOpacity": 10<br>            }<br>          }<br>        }<br>      },<br>      {<br>        "id": 4,<br>        "title": "Tests by Status",<br>        "type": "piechart",<br>        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0},<br>        "targets": [<br>          {<br>            "expr": "sum by (status) (rate(tests_total[5m]))",<br>            "legendFormat": "{{status}}"<br>          }<br>        ]<br>      }<br>    ],<br>    "time": {<br>      "from": "now-1h",<br>      "to": "now"<br>    },<br>    "refresh": "5s"<br>  }<br>}<br></code></pre><br><br>#### Provisioning automatique<br><br><pre><code>yaml<br><h1>grafana/provisioning/dashboards/dashboard.yml</h1><br>apiVersion: 1<br><br>providers:<br>  - name: 'default'<br>    orgId: 1<br>    folder: ''<br>    type: file<br>    disableDeletion: false<br>    updateIntervalSeconds: 10<br>    allowUiUpdates: true<br>    options:<br>      path: /var/lib/grafana/dashboards<br></code></pre><br><br><h3>Alerting avec Grafana</h3><br><br>#### Configuration d'alerte<br><br><pre><code>json<br>{<br>  "alert": {<br>    "id": 1,<br>    "name": "High Test Failure Rate",<br>    "message": "Test failure rate is above 10%",<br>    "frequency": "10s",<br>    "conditions": [<br>      {<br>        "query": {<br>          "queryType": "",<br>          "refId": "A",<br>          "model": {<br>            "expr": "rate(tests_total{status=\"failed\"}[5m]) / rate(tests_total[5m]) * 100",<br>            "interval": "",<br>            "legendFormat": "",<br>            "refId": "A"<br>          }<br>        },<br>        "reducer": {<br>          "type": "last",<br>          "params": []<br>        },<br>        "evaluator": {<br>          "params": [10],<br>          "type": "gt"<br>        }<br>      }<br>    ],<br>    "executionErrorState": "alerting",<br>    "noDataState": "no_data",<br>    "for": "1m"<br>  }<br>}<br></code></pre><br><br>#### Notification channels<br><br><pre><code>json<br>{<br>  "name": "slack-alerts",<br>  "type": "slack",<br>  "settings": {<br>    "url": "https://hooks.slack.com/services/...",<br>    "channel": "#test-alerts",<br>    "username": "Grafana",<br>    "title": "Test Alert",<br>    "text": "{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}"<br>  }<br>}<br></code></pre><br><br><h2>4.4 Intégration des Trois Outils</h2><br><br><h3>Architecture complète</h3><br><br><pre><code>mermaid<br>graph TB<br>    A[Tests Automatisés] --> B[Allure Results]<br>    A --> C[Prometheus Metrics]<br>    <br>    B --> D[Allure Report]<br>    C --> E[Prometheus Server]<br>    <br>    E --> F[Grafana Dashboard]<br>    E --> G[AlertManager]<br>    <br>    G --> H[Slack/Email]<br>    F --> I[Team Dashboard]<br>    D --> J[Detailed Reports]<br>    <br>    K[CI/CD Pipeline] --> A<br>    K --> L[Report Deployment]<br>    L --> D<br></code></pre><br><br><h3>Script d'orchestration</h3><br><br><pre><code>bash<br>#!/bin/bash<br><h1>orchestrate-monitoring.sh</h1><br><br>set -e<br><br>echo "🚀 Démarrage de la stack de monitoring..."<br><br><h1>Démarrage des services</h1><br>docker-compose up -d prometheus grafana<br><br><h1>Attente que les services soient prêts</h1><br>echo "⏳ Attente des services..."<br>sleep 30<br><br><h1>Vérification de Prometheus</h1><br>if curl -f http://localhost:9090/-/healthy; then<br>    echo "✅ Prometheus est opérationnel"<br>else<br>    echo "❌ Erreur: Prometheus n'est pas accessible"<br>    exit 1<br>fi<br><br><h1>Vérification de Grafana</h1><br>if curl -f http://localhost:3000/api/health; then<br>    echo "✅ Grafana est opérationnel"<br>else<br>    echo "❌ Erreur: Grafana n'est pas accessible"<br>    exit 1<br>fi<br><br><h1>Exécution des tests avec génération des métriques</h1><br>echo "🧪 Exécution des tests..."<br>npm test<br><br><h1>Génération du rapport Allure</h1><br>echo "📊 Génération du rapport Allure..."<br>allure generate allure-results --clean -o allure-report<br><br><h1>Ouverture des dashboards</h1><br>echo "🌐 Ouverture des dashboards..."<br>open http://localhost:3000  # Grafana<br>open http://localhost:9090  # Prometheus<br>allure open allure-report   # Allure<br><br>echo "✨ Stack de monitoring prête !"<br>echo "📊 Grafana: http://localhost:3000 (admin/admin123)"<br>echo "🔍 Prometheus: http://localhost:9090"<br>echo "📈 Allure Report: Ouvert automatiquement"<br></code></pre><br><br><h2>Points Clés à Retenir</h2><br><br><li><strong>Allure</strong> excelle dans le reporting détaillé avec une interface utilisateur riche</li><br><li><strong>Prometheus</strong> est idéal pour la collecte et le stockage de métriques temporelles</li><br><li><strong>Grafana</strong> offre des capacités de visualisation et d'alerting puissantes</li><br><li>L'intégration des trois outils crée un écosystème complet de monitoring</li><br><li>Automatiser le déploiement et la configuration pour une adoption facile</li><br><li>Adapter les dashboards aux besoins spécifiques de chaque équipe</li><br><li>Maintenir un équilibre entre détail et lisibilité dans les rapports</li><br><br><h1>Support Théorique - Module 4 : Documentation et Monitoring</h1><br><br><h2>Vue d'ensemble</h2><br><br>Ce module couvre les aspects essentiels de la documentation, du reporting et du monitoring des tests automatisés. Il s'agit d'un module de synthèse qui consolide les apprentissages des modules précédents en se concentrant sur la visibilité, la traçabilité et l'amélioration continue des processus de test.<br><br><h2>Objectifs pédagogiques</h2><br><br>À l'issue de ce module, les apprenants seront capables de :<br><br>1. <strong>Documenter efficacement</strong> les tests automatisés<br>   - Appliquer les standards de documentation et bonnes pratiques<br>   - Utiliser les techniques de nommage et d'annotation appropriées<br>   - Créer une documentation technique claire et maintenable<br>   - Générer automatiquement la documentation à partir du code<br><br>2. <strong>Générer et analyser des rapports</strong> de tests<br>   - Configurer des outils de reporting avancés<br>   - Interpréter les métriques de qualité et de performance<br>   - Analyser les tendances et identifier les patterns d'échec<br>   - Automatiser la génération et distribution des rapports<br><br>3. <strong>Mettre en place un monitoring</strong> des tests<br>   - Configurer des dashboards de suivi en temps réel<br>   - Implémenter des alertes pertinentes et éviter la fatigue d'alerte<br>   - Surveiller les performances et la stabilité des tests<br>   - Adopter une approche proactive du monitoring<br><br>4. <strong>Utiliser les outils spécialisés</strong><br>   - Maîtriser Allure Report pour le reporting visuel interactif<br>   - Configurer Prometheus pour la collecte de métriques temporelles<br>   - Créer des dashboards personnalisés avec Grafana<br>   - Intégrer ces outils dans un écosystème cohérent<br><br><h2>Structure du contenu</h2><br><br><h3>[1. Documentation des Tests Automatisés](01-documentation-tests-automatises.md)</h3><br><strong>Durée : 30 minutes</strong><br><br><li><strong>Importance de la documentation</strong> : Impact sur la maintenabilité et la collaboration</li><br><li><strong>Standards et bonnes pratiques</strong> : Niveaux de documentation, nommage, structure</li><br><li><strong>Documentation du code de test</strong> : Annotations, métadonnées, données de test</li><br><li><strong>Documentation des résultats</strong> : Rapports structurés et exploitables</li><br><li><strong>Outils de documentation</strong> : JSDoc, Sphinx, générateurs automatiques</li><br><br><strong>Points clés :</strong><br><li>La documentation est un investissement qui améliore la maintenabilité</li><br><li>Utiliser des standards cohérents et descriptifs</li><br><li>Automatiser la génération de documentation</li><br><br><h3>[2. Reporting et Analyse des Résultats](02-reporting-analyse-resultats.md)</h3><br><strong>Durée : 45 minutes</strong><br><br><li><strong>Types de rapports</strong> : Temps réel, post-exécution, par audience</li><br><li><strong>Métriques importantes</strong> : Performance, qualité, stabilité, couverture</li><br><li><strong>Analyse des tendances</strong> : Suivi historique, détection d'anomalies</li><br><li><strong>Outils de reporting</strong> : Allure, ReportPortal, TestRail</li><br><li><strong>Automatisation du reporting</strong> : Intégration CI/CD, scripts personnalisés</li><br><li><strong>Analyse des échecs</strong> : Catégorisation automatique, détection de patterns</li><br><br><strong>Points clés :</strong><br><li>Adapter les rapports à l'audience cible</li><br><li>Suivre les métriques clés et analyser les tendances</li><br><li>Automatiser la génération et distribution des rapports</li><br><br><h3>[3. Monitoring des Tests avec Dashboards](03-monitoring-dashboards.md)</h3><br><strong>Durée : 30 minutes</strong><br><br><li><strong>Principes du monitoring</strong> : Approche proactive vs réactive</li><br><li><strong>Métriques de monitoring</strong> : Performance, qualité, métier</li><br><li><strong>Architecture de monitoring</strong> : Stack moderne, collecte de métriques</li><br><li><strong>Dashboards avec Grafana</strong> : Configuration, visualisation, personnalisation</li><br><li><strong>Alerting et notifications</strong> : Configuration d'alertes, channels de notification</li><br><li><strong>Monitoring multi-environnements</strong> : Surveillance de la santé des environnements</li><br><br><strong>Points clés :</strong><br><li>Le monitoring proactif permet de détecter les problèmes avant impact</li><br><li>Utiliser des métriques variées et pertinentes</li><br><li>Configurer des alertes intelligentes</li><br><br><h3>[4. Outils : Allure, Grafana, Prometheus](04-outils-allure-grafana-prometheus.md)</h3><br><strong>Durée : 45 minutes</strong><br><br><li><strong>Allure Report</strong> : Installation, configuration, utilisation avancée, intégration CI/CD</li><br><li><strong>Prometheus</strong> : Collecte de métriques, exposition, requêtes PromQL</li><br><li><strong>Grafana</strong> : Dashboards interactifs, alerting, sources de données multiples</li><br><li><strong>Intégration des outils</strong> : Architecture complète, orchestration, bonnes pratiques</li><br><br><strong>Points clés :</strong><br><li>Chaque outil excelle dans son domaine spécifique</li><br><li>L'intégration crée un écosystème complet de monitoring</li><br><li>Automatiser le déploiement et la configuration</li><br><br><h2>Progression pédagogique</h2><br><br><pre><code>mermaid<br>graph LR<br>    A[Documentation] --> B[Reporting]<br>    B --> C[Monitoring]<br>    C --> D[Outils Intégrés]<br>    <br>    A1[Standards] --> A2[Code de test] --> A3[Résultats]<br>    B1[Types de rapports] --> B2[Métriques] --> B3[Analyse]<br>    C1[Principes] --> C2[Dashboards] --> C3[Alerting]<br>    D1[Allure] --> D2[Prometheus] --> D3[Grafana] --> D4[Intégration]<br></code></pre><br><br><h2>Prérequis techniques</h2><br><br><li><strong>Modules précédents</strong> : Complétion des modules 1, 2 et 3</li><br><li><strong>Connaissances de base</strong> :</li><br>  - Tests automatisés et frameworks de test<br>  - Concepts de monitoring et métriques<br>  - Docker et containerisation<br>  - Outils CI/CD (GitHub Actions, Jenkins)<br><li><strong>Environnement technique</strong> :</li><br>  - Docker et Docker Compose<br>  - Node.js ou Python pour les exemples<br>  - Accès à un navigateur web moderne<br><br><h2>Matériel pédagogique</h2><br><br><h3>Supports visuels</h3><br><li>Diagrammes d'architecture de monitoring</li><br><li>Captures d'écran des interfaces Allure, Grafana, Prometheus</li><br><li>Exemples de dashboards et rapports</li><br><li>Schémas de flux de données</li><br><br><h3>Exemples pratiques</h3><br><li>Configuration complète d'une stack de monitoring</li><br><li>Scripts d'automatisation et d'orchestration</li><br><li>Templates de dashboards et rapports</li><br><li>Exemples de code instrumenté avec métriques</li><br><br><h3>Ressources complémentaires</h3><br><li>[Documentation officielle Allure](https://docs.qameta.io/allure/)</li><br><li>[Guide Prometheus](https://prometheus.io/docs/)</li><br><li>[Tutoriels Grafana](https://grafana.com/tutorials/)</li><br><li>[Bonnes pratiques de monitoring](https://sre.google/sre-book/)</li><br><br><h2>Évaluation des acquis</h2><br><br>Les connaissances seront évaluées à travers :<br><li><strong>QCM intermédiaire</strong> : 6 questions sur les concepts clés</li><br><li><strong>Exercices pratiques</strong> : Configuration d'Allure et dashboards Grafana</li><br><li><strong>Projet intégrateur</strong> : Mise en place d'une stack complète de monitoring</li><br><br><h2>Durée totale estimée</h2><br><br><li><strong>Théorie</strong> : 2h30 (répartie sur 4 sections)</li><br><li><strong>Démonstrations</strong> : 30 minutes</li><br><li><strong>Questions/Discussions</strong> : 30 minutes</li><br><li><strong>Total</strong> : 3h30 (ajustable selon le rythme du groupe)</li><br><br><h2>Notes pour le formateur</h2><br><br><h3>Points d'attention</h3><br><li>Insister sur l'aspect pratique et l'applicabilité immédiate</li><br><li>Montrer des exemples concrets tirés de projets réels</li><br><li>Adapter les exemples aux technologies utilisées par les apprenants</li><br><li>Prévoir du temps pour les questions sur l'intégration dans leurs contextes</li><br><br><h3>Démonstrations recommandées</h3><br>1. <strong>Configuration d'Allure</strong> : Depuis l'installation jusqu'au premier rapport<br>2. <strong>Dashboard Grafana en live</strong> : Création d'un dashboard simple avec métriques réelles<br>3. <strong>Intégration CI/CD</strong> : Déploiement automatique de rapports dans un pipeline<br><br><h3>Variantes selon l'audience</h3><br><li><strong>Développeurs</strong> : Focus sur l'instrumentation du code et l'automatisation</li><br><li><strong>QA/Testeurs</strong> : Emphasis sur l'analyse des rapports et l'interprétation des métriques</li><br><li><strong>DevOps/SRE</strong> : Concentration sur l'architecture de monitoring et l'alerting</li><br><br>\newpage<br><br><h1>Exercices Pratiques</h1><br><br><h1>Exercices Pratiques - Module 4 : Documentation et Monitoring</h1><br><br><h2>Vue d'ensemble</h2><br><br>Ce module propose 2 exercices pratiques pour mettre en application les concepts de documentation, reporting et monitoring des tests automatisés.<br><br><h2>Liste des exercices</h2><br><br><h3>[Exercice 4.1 - Génération de rapports avec Allure Report](exercice-4.1-allure-report/)</h3><br><strong>Durée estimée :</strong> 45 minutes  <br><strong>Difficulté :</strong> Intermédiaire  <br><strong>Objectifs :</strong><br><li>Configurer Allure Report dans un projet de test</li><br><li>Instrumenter les tests avec des annotations Allure</li><br><li>Générer et analyser des rapports visuels</li><br><li>Intégrer Allure dans un pipeline CI/CD</li><br><br><h3>[Exercice 4.2 - Configuration de dashboards avec Grafana et Prometheus](exercice-4.2-grafana-prometheus/)</h3><br><strong>Durée estimée :</strong> 60 minutes  <br><strong>Difficulté :</strong> Avancé  <br><strong>Objectifs :</strong><br><li>Configurer une stack Prometheus/Grafana</li><br><li>Exposer des métriques de tests personnalisées</li><br><li>Créer des dashboards de monitoring</li><br><li>Configurer des alertes sur les métriques de tests</li><br><br><h2>Prérequis techniques</h2><br><br><li>Docker et Docker Compose installés</li><br><li>Node.js (version 16+) ou Python (version 3.8+)</li><br><li>Navigateur web moderne</li><br><li>Éditeur de code (VS Code recommandé)</li><br><br><h2>Structure des exercices</h2><br><br>Chaque exercice contient :<br><li><code>README.md</code> : Instructions détaillées</li><br><li><code>ressources/</code> : Fichiers de base et configuration</li><br><li><code>solution/</code> : Solution complète avec explications</li><br><br><h2>Conseils généraux</h2><br><br>1. <strong>Lisez entièrement</strong> les instructions avant de commencer<br>2. <strong>Testez régulièrement</strong> vos configurations<br>3. <strong>Consultez la documentation</strong> des outils en cas de problème<br>4. <strong>N'hésitez pas</strong> à adapter les exemples à votre contexte<br><br><h2>Support</h2><br><br>En cas de difficulté :<br>1. Vérifiez les prérequis techniques<br>2. Consultez les logs d'erreur<br>3. Référez-vous à la solution fournie<br>4. Demandez de l'aide au formateur<br><br>
</body>
</html>